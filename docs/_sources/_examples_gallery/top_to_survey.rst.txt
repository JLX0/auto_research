
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples_gallery/top_to_survey.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download__examples_gallery_top_to_survey.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_gallery_top_to_survey.py:


.. _top_to_survey_page:

Automatically converts a topic or question of interests into a survey over relevant papers
==========================================

When searching for research papers, the results from a search engine can vary significantly
depending on the specific keywords used, even if those keywords are conceptually similar.
For instance, searching for "LLMs" versus "Large Language Models" may yield different sets
of papers. Additionally, when experimenting with new keywords, it can be challenging to
remember whether a particular paper has already been checked. Furthermore, the process
of downloading papers and organizing them with appropriate filenames can be tedious and
time-consuming.

The function `topic_to_survey` streamlines the entire process by automating several key tasks.
It suggests multiple related keywords to ensure comprehensive coverage of the topic,
merges duplicate results to avoid redundancy, automatically names downloaded files
using the paper titles for easy reference, and automatically rank the paper based their impacts
(see :mod:`auto_research.search.core.AutoSearch.score_threshold`). Moreover, it leverages LLMs
to generate summaries of each paper, saving researchers valuable time and effort.

This script demonstrates the usage of the `topic_to_survey` function from the :mod:`auto_research.applications.surveys` module to:

- Conduct an automated research process based on a user-provided topic.
- Generate and refine a list of keywords for searching research articles.
- Retrieve and download articles based on the specified search criteria.
- Rank, organize and summarize the downloaded articles.
- Check the code availability of the summarized articles (optional).

To get started with the package, you need to set up API keys. For detailed instructions, see :ref:`setting_up_api_keys`.

This script assumes that:

- A valid `key.json` file is available (located at the current working directory (""))

The process involves user interaction, including selecting keywords, summarizing articles, and optionally checking code availability.

Below is an example output from the following input:

- generate code with LLMs
- select
- 1,3
- select
- 2,3
- yes

.. GENERATED FROM PYTHON SOURCE LINES 49-120




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Please enter your research topic or question (e.g., 'Applications of AI in healthcare'): Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed

    Suggested keywords for searching articles based on your input:
    1. generate code with LLMs
    2. code generation with large language models
    3. automated code generation
    4. large language models for coding
    5. program synthesis with LLMs
    6. code synthesis using large language models
    7. neural network based code generation
    8. language model based programming
    9. artificial intelligence in software development
    10. machine learning for code generation

    How would you like to proceed with the suggested keywords?
    1. 'all': Use all the suggested keywords for searching.
    2. 'select': Choose specific keywords by their ranks.
    3. 'custom': Enter your own list of keywords manually.

    Choose an option ('all', 'select', or 'custom'): 
    Available keywords with their ranks:
    1. generate code with LLMs
    2. code generation with large language models
    3. automated code generation
    4. large language models for coding
    5. program synthesis with LLMs
    6. code synthesis using large language models
    7. neural network based code generation
    8. language model based programming
    9. artificial intelligence in software development
    10. machine learning for code generation

    Enter the ranks of the keywords you want to use, separated by commas (e.g., 1,3,5): 
    Using the following keywords: ['generate code with LLMs', 'automated code generation']

    Final keywords to search: ['generate code with LLMs', 'automated code generation']
    ------Searching for the 1th keyword 'generate code with LLMs'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:06<00:24,  6.23s/it]    Searching papers:  40%|████      | 2/5 [00:12<00:19,  6.46s/it]    Searching papers:  60%|██████    | 3/5 [00:18<00:12,  6.17s/it]    Searching papers:  80%|████████  | 4/5 [00:23<00:05,  5.48s/it]    Searching papers: 100%|██████████| 5/5 [00:26<00:00,  4.71s/it]    Searching papers: 100%|██████████| 5/5 [00:26<00:00,  5.29s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 61.25312492028468
    Citation count: 693
    Year of publication: 2024
    Publication venue: Neural Information Processing Systems
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: Codeplan: Repository-level coding using llms and planning
    Abstract:

    Large Language Models (LLMs) demonstrate strong abilities in common-sense
    reasoning and interactive decision-making, but often struggle with complex,
    long-horizon planning tasks. Recent techniques have sought to structure LLM
    outputs using control flow and other code-adjacent techniques to improve
    planning performance. These techniques include using variables (to track
    important information) and functions (to divide complex tasks into smaller
    re-usable sub-tasks). However, purely code-based approaches can be error-prone
    and insufficient for handling ambiguous or unstructured data. To address these
    challenges, we propose REPL-Plan, an LLM planning approach that is fully
    code-expressive (it can utilize all the benefits of code) while also being
    dynamic (it can flexibly adapt from errors and use the LLM for fuzzy
    situations). In REPL-Plan, an LLM solves tasks by interacting with a
    Read-Eval-Print Loop (REPL), which iteratively executes and evaluates code,
    similar to language shells or interactive code notebooks, allowing the model to
    flexibly correct errors and handle tasks dynamically. We demonstrate that
    REPL-Plan achieves strong results across various planning domains compared to
    previous methods.
    Combined Score: 5.480077554195743
    Citation count: 62
    Year of publication: 2024
    Publication venue: Proc. ACM Softw. Eng.
    Authors: R Bairi, A Sonwane, A Kanade, A Iyer


    Link: https://dl.acm.org/doi/pdf/10.1145/3643757
    ArXiv Link: http://arxiv.org/pdf/2411.13826v1
    Downloading Codeplan Repository-level coding using llms and planning.pdf... with upper time limit: 10 seconds
    Downloaded: Codeplan Repository-level coding using llms and planning.pdf.


    Paper 3:
    Title: Llms for science: Usage for code generation and data analysis
    Abstract:

    Large Language Models (LLMs) have shown great potential in code generation.
    However, current LLMs still cannot reliably generate correct code. Moreover, it
    is unclear what kinds of code generation errors LLMs can make. To address this,
    we conducted an empirical study to analyze incorrect code snippets generated by
    six popular LLMs on the HumanEval dataset. We analyzed these errors alongside
    two dimensions of error characteristics -- semantic characteristics and
    syntactic characteristics -- to derive a comprehensive code generation error
    taxonomy for LLMs through open coding and thematic analysis. We then labeled
    all 557 incorrect code snippets based on this taxonomy. Our results showed that
    the six LLMs exhibited similar distributions of syntactic characteristics while
    different distributions of semantic characteristics. Furthermore, we analyzed
    the correlation between different error characteristics and factors such as
    task complexity, code length, and test-pass rate. Finally, we highlight the
    challenges that LLMs may encounter when generating code and propose
    implications for future research on reliable code generation with LLMs.
    Combined Score: 0.8553337321327789
    Citation count: 40
    Year of publication: 2023
    Publication venue: Journal of Software: Evolution and Process
    Authors: M Nejjar, L Zacharias, F Stiehle


    Link: https://onlinelibrary.wiley.com/doi/full/10.1002/smr.2723
    ArXiv Link: http://arxiv.org/pdf/2406.08731v2
    Downloading Llms for science Usage for code generation and data analysis.pdf... with upper time limit: 10 seconds
    Failed to download Llms for science Usage for code generation and data analysis.pdf from https://onlinelibrary.wiley.com/doi/full/10.1002/smr.2723: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/full/10.1002/smr.2723
    Trying to download from ArXiv link: http://arxiv.org/pdf/2406.08731v2
    Downloading Llms for science Usage for code generation and data analysis.pdf... with upper time limit: 10 seconds
    Downloaded: Llms for science Usage for code generation and data analysis.pdf.


    Paper 4:
    Title: Generate and pray: Using sallms to evaluate the security of llm generated code
    Abstract:

    With the growing popularity of Large Language Models (LLMs) in software
    engineers' daily practices, it is important to ensure that the code generated
    by these tools is not only functionally correct but also free of
    vulnerabilities. Although LLMs can help developers to be more productive, prior
    empirical studies have shown that LLMs can generate insecure code. There are
    two contributing factors to the insecure code generation. First, existing
    datasets used to evaluate LLMs do not adequately represent genuine software
    engineering tasks sensitive to security. Instead, they are often based on
    competitive programming challenges or classroom-type coding tasks. In
    real-world applications, the code produced is integrated into larger codebases,
    introducing potential security risks. Second, existing evaluation metrics
    primarily focus on the functional correctness of the generated code while
    ignoring security considerations. Therefore, in this paper, we described SALLM,
    a framework to benchmark LLMs' abilities to generate secure code
    systematically. This framework has three major components: a novel dataset of
    security-centric Python prompts, configurable assessment techniques to evaluate
    the generated code, and novel metrics to evaluate the models' performance from
    the perspective of secure code generation.
    Combined Score: 0.49181689597634787
    Citation count: 23
    Year of publication: 2023
    Publication venue: arXiv preprint arXiv:2311.00889
    Authors: ML Siddiq, JCS Santos


    Link: https://lsiddiqsunny.github.io/public/2311.00889.pdf
    ArXiv Link: http://arxiv.org/pdf/2311.00889v3
    Downloading Generate and pray Using sallms to evaluate the security of llm generated code.pdf... with upper time limit: 10 seconds
    Downloaded: Generate and pray Using sallms to evaluate the security of llm generated code.pdf.


    Paper 5:
    Title: A Review on Code Generation with LLMs: Application and Evaluation
    Abstract:

    Code review is a standard practice for ensuring the quality of software
    projects, and recent research has focused extensively on automated code review.
    While significant advancements have been made in generating code reviews, the
    automated assessment of these reviews remains less explored, with existing
    approaches and metrics often proving inaccurate. Current metrics, such as BLEU,
    primarily rely on lexical similarity between generated and reference reviews.
    However, such metrics tend to underestimate reviews that articulate the
    expected issues in ways different from the references. In this paper, we
    explore how semantic similarity between generated and reference reviews can
    enhance the automated assessment of code reviews. We first present a benchmark
    called \textit{GradedReviews}, which is constructed by collecting real-world
    code reviews from open-source projects, generating reviews using
    state-of-the-art approaches, and manually assessing their quality. We then
    evaluate existing metrics for code review assessment using this benchmark,
    revealing their limitations. To address these limitations, we propose two novel
    semantic-based approaches for assessing code reviews. The first approach
    involves converting both the generated review and its reference into digital
    vectors using a deep learning model and then measuring their semantic
    similarity through Cosine similarity. The second approach generates a prompt
    based on the generated review and its reference, submits this prompt to
    ChatGPT, and requests ChatGPT to rate the generated review according to
    explicitly defined criteria. Our evaluation on the \textit{GradedReviews}
    benchmark indicates that the proposed semantic-based approaches significantly
    outperform existing state-of-the-art metrics in assessing generated code
    review, improving the correlation coefficient between the resulting scores and
    human scores from 0.22 to 0.47.
    Combined Score: 0.4704335526730284
    Citation count: 22
    Year of publication: 2023
    Publication venue: 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)
    Authors: J Wang, Y Chen


    Link: https://ieeexplore.ieee.org/abstract/document/10403378/
    ArXiv Link: http://arxiv.org/pdf/2501.05176v1
    Downloading A Review on Code Generation with LLMs Application and Evaluation.pdf... with upper time limit: 10 seconds
    Failed to download A Review on Code Generation with LLMs Application and Evaluation.pdf from https://ieeexplore.ieee.org/abstract/document/10403378/: 418 Client Error: Unknown Code for url: https://ieeexplore.ieee.org/abstract/document/10403378/
    Trying to download from ArXiv link: http://arxiv.org/pdf/2501.05176v1
    Downloading A Review on Code Generation with LLMs Application and Evaluation.pdf... with upper time limit: 10 seconds
    Downloaded: A Review on Code Generation with LLMs Application and Evaluation.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    ------Searching for the 2th keyword 'automated code generation'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:04<00:19,  4.80s/it]    Searching papers:  40%|████      | 2/5 [00:13<00:21,  7.31s/it]    Searching papers:  60%|██████    | 3/5 [00:17<00:11,  5.70s/it]    Searching papers:  80%|████████  | 4/5 [00:22<00:05,  5.28s/it]    Searching papers: 100%|██████████| 5/5 [00:26<00:00,  4.98s/it]    Searching papers: 100%|██████████| 5/5 [00:26<00:00,  5.35s/it]


    Paper 1:
    Title: Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation
    Abstract:

    This paper presents a comprehensive evaluation of the code generation
    capabilities of ChatGPT, a prominent large language model, compared to human
    programmers. A novel dataset of 131 code-generation prompts across 5 categories
    was curated to enable robust analysis. Code solutions were generated by both
    ChatGPT and humans for all prompts, resulting in 262 code samples. A meticulous
    manual assessment methodology prioritized evaluating correctness,
    comprehensibility, and security using 14 established code quality metrics. The
    key findings reveal ChatGPT's strengths in crafting concise, efficient code
    with advanced constructs, showcasing strengths in data analysis tasks (93.1%
    accuracy) but limitations in visual-graphical challenges. Comparative analysis
    with human code highlights ChatGPT's inclination towards modular design and
    superior error handling. Additionally, machine learning models effectively
    distinguished ChatGPT from human code with up to 88% accuracy, suggesting
    detectable coding style disparities. By providing profound insights into
    ChatGPT's code generation capabilities and limitations through quantitative
    metrics and qualitative analysis, this study makes valuable contributions
    toward advancing AI-based programming assistants. The curated dataset and
    methodology offer a robust foundation for future research in this nascent
    domain. All data and codes are available on
    https://github.com/DSAatUSU/ChatGPT-promises-and-pitfalls.
    Combined Score: 0.17106674642655578
    Citation count: 8
    Year of publication: 2023
    Publication venue: arXiv.org
    Authors: MFA Khan, M Ramsdell, E Falor, H Karimi


    Link: https://arxiv.org/pdf/2311.02640
    ArXiv Link: http://arxiv.org/pdf/2311.02640v1
    Downloading Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation.pdf... with upper time limit: 10 seconds
    Downloaded: Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation.pdf.


    Paper 2:
    Title: Auto-icon: An automated code generation tool for icon designs assisting in ui development
    Abstract:

    Approximately 50% of development resources are devoted to UI development
    tasks [9]. Occupying a large proportion of development resources, developing
    icons can be a time-consuming task, because developers need to consider not
    only effective implementation methods but also easy-to-understand descriptions.
    In this paper, we present Auto-Icon+, an approach for automatically generating
    readable and efficient code for icons from design artifacts. According to our
    interviews to understand the gap between designers (icons are assembled from
    multiple components) and developers (icons as single images), we apply a
    heuristic clustering algorithm to compose the components into an icon image. We
    then propose an approach based on a deep learning model and computer vision
    methods to convert the composed icon image to fonts with descriptive labels,
    thereby reducing the laborious manual effort for developers and facilitating UI
    development. We quantitatively evaluate the quality of our method in the real
    world UI development environment and demonstrate that our method offers
    developers accurate, efficient, readable, and usable code for icon designs, in
    terms of saving 65.2% implementing time.
    Combined Score: 0.1073312629199899
    Citation count: 30
    Year of publication: 2021
    Publication venue: International Conference on Intelligent User Interfaces
    Authors: S Feng, S Ma, J Yu, C Chen, T Zhou


    Link: https://chunyang-chen.github.io/publication/icon_IUI21.pdf
    ArXiv Link: http://arxiv.org/pdf/2204.08676v1
    Downloading Auto-icon An automated code generation tool for icon designs assisting in ui development.pdf... with upper time limit: 10 seconds
    Downloaded: Auto-icon An automated code generation tool for icon designs assisting in ui development.pdf.


    Paper 3:
    Title: A parallel corpus of python functions and documentation strings for automated code documentation and code generation
    Abstract:

    Automated documentation of programming source code and automated code
    generation from natural language are challenging tasks of both practical and
    scientific interest. Progress in these areas has been limited by the low
    availability of parallel corpora of code and natural language descriptions,
    which tend to be small and constrained to specific domains.
      In this work we introduce a large and diverse parallel corpus of a hundred
    thousands Python functions with their documentation strings ("docstrings")
    generated by scraping open source repositories on GitHub. We describe baseline
    results for the code documentation and code generation tasks obtained by neural
    machine translation. We also experiment with data augmentation techniques to
    further increase the amount of training data.
      We release our datasets and processing scripts in order to stimulate research
    in these areas.
    Combined Score: 0.09190672153635117
    Citation count: 201
    Year of publication: 2017
    Publication venue: International Joint Conference on Natural Language Processing
    Authors: AVM Barone, R Sennrich


    Link: https://arxiv.org/pdf/1707.02275
    ArXiv Link: http://arxiv.org/pdf/1707.02275v1
    Downloading A parallel corpus of python functions and documentation strings for automated code documentation and code generation.pdf... with upper time limit: 10 seconds
    Downloaded: A parallel corpus of python functions and documentation strings for automated code documentation and code generation.pdf.


    Paper 4:
    Title: Recent Progress in Automated Code Generation from GUI Images Using Machine Learning Techniques.
    Abstract:

    Recent progress on deep learning has made it possible to automatically
    transform the screenshot of Graphic User Interface (GUI) into code by using the
    encoder-decoder framework. While the commonly adopted image encoder (e.g., CNN
    network), might be capable of extracting image features to the desired level,
    interpreting these abstract image features into hundreds of tokens of code puts
    a particular challenge on the decoding power of the RNN-based code generator.
    Considering the code used for describing GUI is usually hierarchically
    structured, we propose a new attention-based hierarchical code generation
    model, which can describe GUI images in a finer level of details, while also
    being able to generate hierarchically structured code in consistency with the
    hierarchical layout of the graphic elements in the GUI. Our model follows the
    encoder-decoder framework, all the components of which can be trained jointly
    in an end-to-end manner. The experimental results show that our method
    outperforms other current state-of-the-art methods on both a publicly available
    GUI-code dataset as well as a dataset established by our own.
    Combined Score: 0.03591072925376573
    Citation count: 19
    Year of publication: 2020
    Publication venue: Journal of universal computer science (Online)
    Authors: D de Souza Baulé


    Link: https://pdfs.semanticscholar.org/1d21/d4b398a19fbe94fb6881cf437571871a792e.pdf
    ArXiv Link: http://arxiv.org/pdf/1810.11536v1
    Downloading Recent Progress in Automated Code Generation from GUI Images Using Machine Learning Techniques..pdf... with upper time limit: 10 seconds
    Downloaded: Recent Progress in Automated Code Generation from GUI Images Using Machine Learning Techniques..pdf.


    Paper 5:
    Title: Automated code generation for discontinuous Galerkin methods
    Abstract:

    A compiler approach for generating low-level computer code from high-level
    input for discontinuous Galerkin finite element forms is presented. The input
    language mirrors conventional mathematical notation, and the compiler generates
    efficient code in a standard programming language. This facilitates the rapid
    generation of efficient code for general equations in varying spatial
    dimensions. Key concepts underlying the compiler approach and the automated
    generation of computer code are elaborated. The approach is demonstrated for a
    range of common problems, including the Poisson, biharmonic,
    advection--diffusion and Stokes equations.
    Combined Score: 0.0035543588444160337
    Citation count: 72
    Year of publication: 2009
    Publication venue: SIAM Journal on Scientific Computing
    Authors: KB Ølgaard, A Logg, GN Wells


    Link: https://arxiv.org/pdf/1104.0628
    ArXiv Link: http://arxiv.org/pdf/1104.0628v1
    Downloading Automated code generation for discontinuous Galerkin methods.pdf... with upper time limit: 10 seconds
    Downloaded: Automated code generation for discontinuous Galerkin methods.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    Files organized in papers/papers_organized
    Target folder saved to papers/papers_organized.zip
    The entire source folder saved to papers.zip

    How would you like to summarize the papers?
    1. 'all': Summarize all papers in the organized folder.
    2. 'select': Choose specific papers by their ranks to summarize.

    Choose an option ('all' or 'select'): 
    Available papers with their ranks:
    1. 01_61.3_Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf
    2. 02_5.48_Codeplan Repository-level coding using llms and planning.pdf
    3. 03_0.855_Llms for science Usage for code generation and data analysis.pdf
    4. 04_0.492_Generate and pray Using sallms to evaluate the security of llm generated code.pdf
    5. 05_0.47_A Review on Code Generation with LLMs Application and Evaluation.pdf
    6. 06_0.171_Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation.pdf
    7. 07_0.107_Auto-icon An automated code generation tool for icon designs assisting in ui development.pdf
    8. 08_0.0919_A parallel corpus of python functions and documentation strings for automated code documentation and code generation.pdf
    9. 09_0.0359_Recent Progress in Automated Code Generation from GUI Images Using Machine Learning Techniques..pdf
    10. 10_0.00355_Automated code generation for discontinuous Galerkin methods.pdf

    Enter the ranks of the papers you want to summarize, separated by commas (e.g., 1,3,5): 
    Summarizing the following papers: ['02_5.48_Codeplan Repository-level coding using llms and planning.pdf', '03_0.855_Llms for science Usage for code generation and data analysis.pdf']

    Processing file: 02_5.48_Codeplan Repository-level coding using llms and planning.pdf
    Begin analyzing the article located at papers/papers_organized/02_5.48_Codeplan Repository-level coding using llms and planning.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper presents CodePlan, a neuro-symbolic framework designed to automate complex repository-level coding tasks in software engineering, allowing for multi-step code edits across interdependent files in large codebases.

    2. Existing problems: Traditional coding tools powered by Large Language Models (LLMs) primarily focus on localized code edits, failing to handle tasks that require extensive changes across an entire repository with interdependencies between files. This leaves a gap in the automation of more complex coding tasks, resulting in manual effort and potential errors.

    3. The main contributions: CodePlan is the first systematic approach that formalizes repository-level coding as a planning problem. It integrates incremental dependency analysis, change may-impact analysis, and adaptive planning with LLMs to generate a chain of code edits automatically, addressing the challenges of dependencies and extensive edits that previous methods did not tackle.

    4. Experimental results: CodePlan was evaluated on complex tasks such as package migration in C# and temporal code edits in Python across multiple repositories (2–97 files per repository). It outperformed baseline methods, achieving valid builds in 5 out of 7 repositories, while the baseline approaches failed to validate any, showcasing substantial improvements in accuracy and effectiveness.

    5. Conclusions: The findings indicate that CodePlan effectively automates repository-level coding tasks, promising significant implications for software engineering. Future directions include expanding its capabilities for more programming languages, optimizing its editing strategies, and conducting large-scale experiments to ascertain its effectiveness across a broader range of coding challenges.
    The total cost is 0.0037180499999999997 USD

    Processing file: 03_0.855_Llms for science Usage for code generation and data analysis.pdf
    Begin analyzing the article located at papers/papers_organized/03_0.855_Llms for science Usage for code generation and data analysis.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper focuses on analyzing the code generation errors produced by Large Language Models (LLMs) through an empirical study using the HumanEval dataset to develop a comprehensive taxonomy of these errors.

    2. Existing problems: Despite advancements in LLMs for code generation, there is a lack of understanding regarding the types and causes of errors these models produce. Prior research has not effectively analyzed the reliability of LLM-generated code and has overlooked the differences in error characteristics across various models.

    3. The main contributions: The authors established a taxonomy categorizing code generation errors based on syntactic and semantic characteristics, enhancing understanding of LLM limitations. They also compared error distributions among different LLMs, discussed bug-fixing efforts, and established a website for interactive analysis of code generation errors.

    4. Experimental results: The study involved six LLMs that generated 557 incorrect code snippets across 164 tasks in the HumanEval dataset. It was found that while the syntactic errors were similar across models, the semantic errors varied significantly, indicating differences in how well each LLM interprets task requirements.

    5. Conclusions: The study revealed that most incorrect code is compilable but erroneous in logic, indicating that while LLMs have learned the syntactic rules, they struggle with interpreting complex natural language and generating nuanced code. The paper suggests further research into improving LLMs for more reliable code generation and highlights the need for robust testing and code reviews to detect subtle logical errors.
    The total cost is 0.0038998499999999994 USD

    Total cost for summarizing all files: 0.007617899999999999

    The summaries for all selected files are printed below:
    ------Paper title: 02_5.48_Codeplan Repository-level coding using llms and planning.pdf------

    1. The main topic: The paper presents CodePlan, a neuro-symbolic framework designed to automate complex repository-level coding tasks in software engineering, allowing for multi-step code edits across interdependent files in large codebases.

    2. Existing problems: Traditional coding tools powered by Large Language Models (LLMs) primarily focus on localized code edits, failing to handle tasks that require extensive changes across an entire repository with interdependencies between files. This leaves a gap in the automation of more complex coding tasks, resulting in manual effort and potential errors.

    3. The main contributions: CodePlan is the first systematic approach that formalizes repository-level coding as a planning problem. It integrates incremental dependency analysis, change may-impact analysis, and adaptive planning with LLMs to generate a chain of code edits automatically, addressing the challenges of dependencies and extensive edits that previous methods did not tackle.

    4. Experimental results: CodePlan was evaluated on complex tasks such as package migration in C# and temporal code edits in Python across multiple repositories (2–97 files per repository). It outperformed baseline methods, achieving valid builds in 5 out of 7 repositories, while the baseline approaches failed to validate any, showcasing substantial improvements in accuracy and effectiveness.

    5. Conclusions: The findings indicate that CodePlan effectively automates repository-level coding tasks, promising significant implications for software engineering. Future directions include expanding its capabilities for more programming languages, optimizing its editing strategies, and conducting large-scale experiments to ascertain its effectiveness across a broader range of coding challenges.



    ------Paper title: 03_0.855_Llms for science Usage for code generation and data analysis.pdf------

    1. The main topic: The paper focuses on analyzing the code generation errors produced by Large Language Models (LLMs) through an empirical study using the HumanEval dataset to develop a comprehensive taxonomy of these errors.

    2. Existing problems: Despite advancements in LLMs for code generation, there is a lack of understanding regarding the types and causes of errors these models produce. Prior research has not effectively analyzed the reliability of LLM-generated code and has overlooked the differences in error characteristics across various models.

    3. The main contributions: The authors established a taxonomy categorizing code generation errors based on syntactic and semantic characteristics, enhancing understanding of LLM limitations. They also compared error distributions among different LLMs, discussed bug-fixing efforts, and established a website for interactive analysis of code generation errors.

    4. Experimental results: The study involved six LLMs that generated 557 incorrect code snippets across 164 tasks in the HumanEval dataset. It was found that while the syntactic errors were similar across models, the semantic errors varied significantly, indicating differences in how well each LLM interprets task requirements.

    5. Conclusions: The study revealed that most incorrect code is compilable but erroneous in logic, indicating that while LLMs have learned the syntactic rules, they struggle with interpreting complex natural language and generating nuanced code. The paper suggests further research into improving LLMs for more reliable code generation and highlights the need for robust testing and code reviews to detect subtle logical errors.




    Would you like to check the code availability of the articles? (yes/no): 
    Checking code availability for the summarized articles...

    Checking code availability for: 02_5.48_Codeplan Repository-level coding using llms and planning.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    https://github.com/microsoft/codeplan
    The total cost is 0.00414465 USD

    Checking code availability for: 03_0.855_Llms for science Usage for code generation and data analysis.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    not available
    The total cost is 0.0033426 USD

    Total cost for checking code availability: 0.007487249999999999 USD
    Total cost for the entire process (summaries + code availability check): 0.015105149999999998 USD






|

.. code-block:: Python


    from auto_research.applications.surveys import topic_to_survey


    if __name__ == "__main__":
        """
        Main execution block for the `topic_to_survey` function.

        This block initializes the `topic_to_survey` function with the specified parameters and runs the automated research process.

        Example:
            # Sample usage:
            topic_to_survey(
                num_results=5,
                sort_by="relevance",
                date_cutoff="2024-12-01",
                score_threshold=0,
                destination_folder="papers",
                model="gpt-4o-mini",
                api_key_path="",
                api_key_type="OpenAI",
                organize_files=True,
                order_by_score=True,
                zip_folder=True,
                api_key=None,  # Directly provide the API key as a string. If None, the key will be retrieved from the file.
            )

        Parameters
        ----------
        num_results : int, optional
            Number of search results to retrieve. Defaults to 30.
        sort_by : str, optional
            Sorting criteria for search results. Options: "relevance", "date". Defaults to "relevance".
        date_cutoff : str, optional
            Cutoff date for search results. Only articles published before this date will be included. Defaults to "2024-12-01". Only relevant when `sort_by` is set as "date".
        score_threshold : float, optional
            Minimum score threshold for articles. Articles with a score below this will be excluded. Defaults to 0.5.
        destination_folder : str, optional
            Folder to store downloaded articles. Defaults to "papers".
        model : str, optional
            Model to use for summarization and keyword suggestions. Defaults to "gpt-4o-mini".
        api_key_path : str, optional
            Path to the directory containing the API key. Defaults to "../". Set it as "" if the file is located at the current directory.
        api_key_type : str, optional
            Type of API key to retrieve. Options: "OpenAI", "DeepSeek". Defaults to "OpenAI".
        organize_files : bool, optional
            Whether to organize the downloaded articles into subfolders based on their rank and score. Defaults to True.
        order_by_score : bool, optional
            Whether to order articles by their score when organizing. Defaults to True.
        zip_folder : bool, optional
            Whether to zip the organized folder after processing. Defaults to True.
        api_key : str, optional
            Directly provide the API key as a string. If None, the key will be retrieved from the file. Defaults to None.

        Returns
        -------
        None
        """
        topic_to_survey(
            num_results=5,
            sort_by="relevance",
            date_cutoff="2024-12-01",
            score_threshold=0,
            destination_folder="papers",
            model="gpt-4o-mini",
            api_key_path="",
            api_key_type="OpenAI",
            organize_files=True,
            order_by_score=True,
            zip_folder=True,
            api_key=None,
        )

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (4 minutes 23.364 seconds)


.. _sphx_glr_download__examples_gallery_top_to_survey.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: top_to_survey.ipynb <top_to_survey.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: top_to_survey.py <top_to_survey.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: top_to_survey.zip <top_to_survey.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
