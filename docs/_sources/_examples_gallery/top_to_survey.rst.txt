
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples_gallery/top_to_survey.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download__examples_gallery_top_to_survey.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_gallery_top_to_survey.py:


.. _top_to_survey_page:

Automatically converts a topic or question of interests into a survey over relevant papers
==========================================

This script demonstrates the usage of the `topic_to_survey` function from the :mod:`auto_research.applications.surveys` module to:

- Conduct an automated research process based on a user-provided topic.
- Generate and refine a list of keywords for searching research articles.
- Retrieve and download articles based on the specified search criteria.
- Organize and summarize the downloaded articles.
- Check the code availability of the summarized articles (optional).

To get started with the package, you need to set up API keys. For detailed instructions, see :ref:`setting_up_api_keys`.

This script assumes that:

- A valid `key.json` file is available (located at the current working directory (""))

The process involves user interaction, including selecting keywords, summarizing articles, and optionally checking code availability.

Below is an example output from the following input:

- generate code with LLMs
- select
- 1,3
- select
- 2,3
- yes

.. GENERATED FROM PYTHON SOURCE LINES 34-105




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Please enter your research topic or question (e.g., 'Applications of AI in healthcare'): Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed

    Suggested keywords for searching articles based on your input:
    1. code generation with language models
    2. code generation with LLMs
    3. code synthesis using language models
    4. automated code generation
    5. artificial intelligence for code generation
    6. code generation using GPT
    7. natural language code generation
    8. generative models for programming
    9. software development with language models
    10. programming assistance using language models

    How would you like to proceed with the suggested keywords?
    1. 'all': Use all the suggested keywords for searching.
    2. 'select': Choose specific keywords by their ranks.
    3. 'custom': Enter your own list of keywords manually.

    Choose an option ('all', 'select', or 'custom'): 
    Available keywords with their ranks:
    1. code generation with language models
    2. code generation with LLMs
    3. code synthesis using language models
    4. automated code generation
    5. artificial intelligence for code generation
    6. code generation using GPT
    7. natural language code generation
    8. generative models for programming
    9. software development with language models
    10. programming assistance using language models

    Enter the ranks of the keywords you want to use, separated by commas (e.g., 1,3,5): 
    Using the following keywords: ['code generation with language models', 'code synthesis using language models']

    Final keywords to search: ['code generation with language models', 'code synthesis using language models']
    ------Searching for the 1th keyword 'code generation with language models'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:03<00:13,  3.43s/it]    Searching papers:  40%|████      | 2/5 [00:07<00:11,  3.81s/it]    Searching papers:  60%|██████    | 3/5 [00:11<00:08,  4.10s/it]    Searching papers:  80%|████████  | 4/5 [00:15<00:03,  3.79s/it]    Searching papers: 100%|██████████| 5/5 [00:19<00:00,  3.89s/it]    Searching papers: 100%|██████████| 5/5 [00:19<00:00,  3.87s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 60.54601813909813
    Citation count: 685
    Year of publication: 2024
    Publication venue: Neural Information Processing Systems
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: Self-planning code generation with large language models
    Abstract:

    Large language models have demonstrated the ability to generate both natural
    language and programming language text. Such models open up the possibility of
    multi-language code generation: could code generation models generalize
    knowledge from one language to another? Although contemporary code generation
    models can generate semantically correct Python code, little is known about
    their abilities with other languages. We propose MultiPL-E, a system for
    translating unit test-driven code generation benchmarks to new languages. We
    create the first massively multilingual code generation benchmark by using
    MultiPL-E to translate two popular Python code generation benchmarks to 18
    additional programming languages.
      We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18
    languages that encompass a range of programming paradigms and popularity. Using
    these new parallel benchmarks, we evaluate the multi-language performance of
    three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We
    find that Codex matches or even exceeds its performance on Python for several
    other languages. The range of programming languages represented in MultiPL-E
    allow us to explore the impact of language frequency and language features on
    model performance. Finally, the MultiPL-E approach of compiling code generation
    benchmarks to new programming languages is both scalable and extensible, making
    it straightforward to evaluate new models, benchmarks, and languages.
    Combined Score: 10.341436674853258
    Citation count: 117
    Year of publication: 2024
    Publication venue: ACM Transactions on Software Engineering and Methodology
    Authors: X Jiang, Y Dong, L Wang, Z Fang, Q Shang


    Link: https://dl.acm.org/doi/full/10.1145/3672456
    ArXiv Link: http://arxiv.org/pdf/2208.08227v4
    Downloading Self-planning code generation with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Self-planning code generation with large language models.pdf.
    /home/j/experiments/auto_research/auto_research/search/files_management.py:56: UserWarning: Error opening PDF: Failed to open file 'papers/Self-planning code generation with large language models.pdf'.
      warnings.warn(f"Error opening PDF: {e}", UserWarning)
    The downloaded PDF file 'Self-planning code generation with large language models.pdf' is corrupted.
    File removed: Self-planning code generation with large language models.pdf
    Trying to download from ArXiv link: http://arxiv.org/pdf/2208.08227v4
    Downloading Self-planning code generation with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Self-planning code generation with large language models.pdf.


    Paper 3:
    Title: Planning with large language models for code generation
    Abstract:

    Developing domain models is one of the few remaining places that require
    manual human labor in AI planning. Thus, in order to make planning more
    accessible, it is desirable to automate the process of domain model generation.
    To this end, we investigate if large language models (LLMs) can be used to
    generate planning domain models from simple textual descriptions. Specifically,
    we introduce a framework for automated evaluation of LLM-generated domains by
    comparing the sets of plans for domain instances. Finally, we perform an
    empirical analysis of 7 large language models, including coding and chat models
    across 9 different planning domains, and under three classes of natural
    language domain descriptions. Our results indicate that LLMs, particularly
    those with high parameter counts, exhibit a moderate level of proficiency in
    generating correct planning domains from natural language descriptions. Our
    code is available at https://github.com/IBM/NL2PDDL.
    Combined Score: 2.8653680026448094
    Citation count: 134
    Year of publication: 2023
    Publication venue: International Conference on Learning Representations
    Authors: S Zhang, Z Chen, Y Shen, M Ding


    Link: https://arxiv.org/pdf/2303.05510
    ArXiv Link: http://arxiv.org/pdf/2405.06650v1
    Downloading Planning with large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Planning with large language models for code generation.pdf.


    Paper 4:
    Title: Synchromesh: Reliable code generation from pre-trained language models
    Abstract:

    Large language models have demonstrated the ability to generate both natural
    language and programming language text. Such models open up the possibility of
    multi-language code generation: could code generation models generalize
    knowledge from one language to another? Although contemporary code generation
    models can generate semantically correct Python code, little is known about
    their abilities with other languages. We propose MultiPL-E, a system for
    translating unit test-driven code generation benchmarks to new languages. We
    create the first massively multilingual code generation benchmark by using
    MultiPL-E to translate two popular Python code generation benchmarks to 18
    additional programming languages.
      We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18
    languages that encompass a range of programming paradigms and popularity. Using
    these new parallel benchmarks, we evaluate the multi-language performance of
    three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We
    find that Codex matches or even exceeds its performance on Python for several
    other languages. The range of programming languages represented in MultiPL-E
    allow us to explore the impact of language frequency and language features on
    model performance. Finally, the MultiPL-E approach of compiling code generation
    benchmarks to new programming languages is both scalable and extensible, making
    it straightforward to evaluate new models, benchmarks, and languages.
    Combined Score: 1.765625
    Citation count: 226
    Year of publication: 2022
    Publication venue: International Conference on Learning Representations
    Authors: G Poesia, O Polozov, V Le, A Tiwari, G Soares


    Link: https://arxiv.org/pdf/2201.11227
    ArXiv Link: http://arxiv.org/pdf/2208.08227v4
    Downloading Synchromesh Reliable code generation from pre-trained language models.pdf... with upper time limit: 10 seconds
    Downloaded: Synchromesh Reliable code generation from pre-trained language models.pdf.


    Paper 5:
    Title: A survey on evaluating large language models in code generation tasks
    Abstract:

    This paper provides a comprehensive review of the current methods and metrics
    used to evaluate the performance of Large Language Models (LLMs) in code
    generation tasks. With the rapid growth in demand for automated software
    development, LLMs have demonstrated significant potential in the field of code
    generation. The paper begins by reviewing the historical development of LLMs
    and their applications in code generation. Next, it details various methods and
    metrics for assessing the code generation capabilities of LLMs, including code
    correctness, efficiency, readability, and evaluation methods based on expert
    review and user experience. The paper also evaluates the widely used benchmark
    datasets, identifying their limitations and proposing directions for future
    improvements. Specifically, the paper analyzes the performance of code
    generation models across different tasks by combining multiple evaluation
    metrics, such as code compilation/interpretation success rates, unit test pass
    rates, and performance and efficiency metrics, to comprehensively assess the
    practical application of LLMs in code generation. Finally, the paper discusses
    the challenges faced in evaluating LLMs in code generation, particularly how to
    ensure the comprehensiveness and accuracy of evaluation methods and how to
    adapt to the evolving practices of software development. These analyses and
    discussions provide valuable insights for further optimizing and improving the
    application of LLMs in code generation tasks.
    Combined Score: 0.44194173824159216
    Citation count: 5
    Year of publication: 2024
    Publication venue: arXiv.org
    Authors: L Chen, Q Guo, H Jia, Z Zeng, X Wang, Y Xu


    Link: https://arxiv.org/pdf/2408.16498
    ArXiv Link: http://arxiv.org/pdf/2408.16498v1
    Downloading A survey on evaluating large language models in code generation tasks.pdf... with upper time limit: 10 seconds
    Downloaded: A survey on evaluating large language models in code generation tasks.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    ------Searching for the 2th keyword 'code synthesis using language models'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:04<00:18,  4.68s/it]    Searching papers:  40%|████      | 2/5 [00:08<00:11,  3.92s/it]    Searching papers:  60%|██████    | 3/5 [00:12<00:08,  4.07s/it]    Searching papers:  80%|████████  | 4/5 [00:15<00:03,  3.64s/it]    Searching papers: 100%|██████████| 5/5 [00:20<00:00,  4.03s/it]    Searching papers: 100%|██████████| 5/5 [00:20<00:00,  4.00s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 60.54601813909813
    Citation count: 685
    Year of publication: 2024
    Publication venue: Neural Information Processing Systems
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: A systematic evaluation of large language models of code
    Abstract:

    Language models (LMs) have exhibited impressive abilities in generating codes
    from natural language requirements. In this work, we highlight the diversity of
    code generated by LMs as a critical criterion for evaluating their code
    generation capabilities, in addition to functional correctness. Despite its
    practical implications, there is a lack of studies focused on assessing the
    diversity of generated code, which overlooks its importance in the development
    of code LMs. We propose a systematic approach to evaluate the diversity of
    generated code, utilizing various metrics for inter-code similarity as well as
    functional correctness. Specifically, we introduce a pairwise code similarity
    measure that leverages large LMs' capabilities in code understanding and
    reasoning, demonstrating the highest correlation with human judgment. We
    extensively investigate the impact of various factors on the quality of
    generated code, including model sizes, temperatures, training approaches,
    prompting strategies, and the difficulty of input problems. Our consistent
    observation of a positive correlation between the test pass score and the
    inter-code similarity score indicates that current LMs tend to produce
    functionally correct code with limited diversity.
    Combined Score: 5.2421875
    Citation count: 671
    Year of publication: 2022
    Publication venue: MAPS@PLDI
    Authors: FF Xu, U Alon, G Neubig, VJ Hellendoorn


    Link: https://dl.acm.org/doi/pdf/10.1145/3520312.3534862
    ArXiv Link: http://arxiv.org/pdf/2408.14504v1
    Downloading A systematic evaluation of large language models of code.pdf... with upper time limit: 10 seconds
    Downloaded: A systematic evaluation of large language models of code.pdf.


    Paper 3:
    Title: Program synthesis with large language models
    Abstract:

    GitHub Copilot, an extension for the Visual Studio Code development
    environment powered by the large-scale language model Codex, makes automatic
    program synthesis available for software developers. This model has been
    extensively studied in the field of deep learning, however, a comparison to
    genetic programming, which is also known for its performance in automatic
    program synthesis, has not yet been carried out. In this paper, we evaluate
    GitHub Copilot on standard program synthesis benchmark problems and compare the
    achieved results with those from the genetic programming literature. In
    addition, we discuss the performance of both approaches. We find that the
    performance of the two approaches on the benchmark problems is quite similar,
    however, in comparison to GitHub Copilot, the program synthesis approaches
    based on genetic programming are not yet mature enough to support programmers
    in practical software development. Genetic programming usually needs a huge
    amount of expensive hand-labeled training cases and takes too much time to
    generate solutions. Furthermore, source code generated by genetic programming
    approaches is often bloated and difficult to understand. For future work on
    program synthesis with genetic programming, we suggest researchers to focus on
    improving the execution time, readability, and usability.
    Combined Score: 4.765508073647552
    Citation count: 1332
    Year of publication: 2021
    Publication venue: arXiv.org
    Authors: J Austin, A Odena, M Nye, M Bosma


    Link: https://arxiv.org/pdf/2108.07732
    ArXiv Link: http://arxiv.org/pdf/2111.07875v1
    Downloading Program synthesis with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Program synthesis with large language models.pdf.


    Paper 4:
    Title: Jigsaw: Large language models meet program synthesis
    Abstract:

    Large pre-trained language models such as GPT-3, Codex, and Google's language
    model are now capable of generating code from natural language specifications
    of programmer intent. We view these developments with a mixture of optimism and
    caution. On the optimistic side, such large language models have the potential
    to improve productivity by providing an automated AI pair programmer for every
    programmer in the world. On the cautionary side, since these large language
    models do not understand program semantics, they offer no guarantees about
    quality of the suggested code. In this paper, we present an approach to augment
    these large language models with post-processing steps based on program
    analysis and synthesis techniques, that understand the syntax and semantics of
    programs. Further, we show that such techniques can make use of user feedback
    and improve with usage. We present our experiences from building and evaluating
    such a tool jigsaw, targeted at synthesizing code for using Python Pandas API
    using multi-modal inputs. Our experience suggests that as these large language
    models evolve for synthesizing code from intent, jigsaw has an important role
    to play in improving the accuracy of the systems.
    Combined Score: 1.7265625
    Citation count: 221
    Year of publication: 2022
    Publication venue: International Conference on Software Engineering
    Authors: N Jain, S Vaidyanath, A Iyer, N Natarajan


    Link: https://arxiv.org/pdf/2112.02969
    ArXiv Link: http://arxiv.org/pdf/2112.02969v1
    Downloading Jigsaw Large language models meet program synthesis.pdf... with upper time limit: 10 seconds
    Downloaded: Jigsaw Large language models meet program synthesis.pdf.


    Paper 5:
    Title: A hazard analysis framework for code synthesis large language models
    Abstract:

    Codex, a large language model (LLM) trained on a variety of codebases,
    exceeds the previous state of the art in its capacity to synthesize and
    generate code. Although Codex provides a plethora of benefits, models that may
    generate code on such scale have significant limitations, alignment problems,
    the potential to be misused, and the possibility to increase the rate of
    progress in technical fields that may themselves have destabilizing impacts or
    have misuse potential. Yet such safety impacts are not yet known or remain to
    be explored. In this paper, we outline a hazard analysis framework constructed
    at OpenAI to uncover hazards or safety risks that the deployment of models like
    Codex may impose technically, socially, politically, and economically. The
    analysis is informed by a novel evaluation framework that determines the
    capacity of advanced code generation techniques against the complexity and
    expressivity of specification prompts, and their capability to understand and
    execute them relative to human ability.
    Combined Score: 0.1953125
    Citation count: 25
    Year of publication: 2022
    Publication venue: arXiv.org
    Authors: H Khlaaf, P Mishkin, J Achiam, G Krueger


    Link: https://www.33wang.com/blogfile/20230424200649628.pdf
    ArXiv Link: http://arxiv.org/pdf/2207.14157v1
    Downloading A hazard analysis framework for code synthesis large language models.pdf... with upper time limit: 10 seconds
    Failed to download A hazard analysis framework for code synthesis large language models.pdf from https://www.33wang.com/blogfile/20230424200649628.pdf: 502 Server Error: Bad Gateway for url: https://www.33wang.com/blogfile/20230424200649628.pdf
    Trying to download from ArXiv link: http://arxiv.org/pdf/2207.14157v1
    Downloading A hazard analysis framework for code synthesis large language models.pdf... with upper time limit: 10 seconds
    Downloaded: A hazard analysis framework for code synthesis large language models.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    Files organized in papers/papers_organized
    Target folder saved to papers/papers_organized.zip
    The entire source folder saved to papers.zip

    How would you like to summarize the papers?
    1. 'all': Summarize all papers in the organized folder.
    2. 'select': Choose specific papers by their ranks to summarize.

    Choose an option ('all' or 'select'): 
    Available papers with their ranks:
    1. 3_5.24_A systematic evaluation of large language models of code.pdf
    2. 9_0.195_A hazard analysis framework for code synthesis large language models.pdf
    3. 4_4.77_Program synthesis with large language models.pdf
    4. 5_2.87_Planning with large language models for code generation.pdf
    5. 1_60.5_Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf
    6. 7_1.73_Jigsaw Large language models meet program synthesis.pdf
    7. 6_1.77_Synchromesh Reliable code generation from pre-trained language models.pdf
    8. 2_10.3_Self-planning code generation with large language models.pdf
    9. 8_0.442_A survey on evaluating large language models in code generation tasks.pdf

    Enter the ranks of the papers you want to summarize, separated by commas (e.g., 1,3,5): 
    Summarizing the following papers: ['9_0.195_A hazard analysis framework for code synthesis large language models.pdf', '4_4.77_Program synthesis with large language models.pdf']

    Processing file: 9_0.195_A hazard analysis framework for code synthesis large language models.pdf
    Begin analyzing the article located at papers/papers_organized/9_0.195_A hazard analysis framework for code synthesis large language models.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper focuses on the safety and hazard analysis of Codex, a large language model (LLM) for code synthesis, including its capabilities and potential risks associated with its deployment in technical and non-technical contexts.

    2. Existing problems: Previous studies have primarily centered on simple code generation tasks and have not adequately addressed the alignment problems, misuse potential, and complex safety implications associated with advanced code synthesis models like Codex. Moreover, standard evaluation metrics have not considered the complexity of real-world coding scenarios or the potential hazards stemming from these models.

    3. The main contributions: The authors propose a novel evaluation framework to assess the generative capabilities of code synthesis LLMs against human ability and complexity of specification prompts. This framework is complemented by a hazard analysis tailored for LLMs, identifying risks associated with technical, social, political, and economic dimensions of deploying such models.

    4. Experimental results: The paper outlines qualitative metrics developed to benchmark the capabilities of Codex against complexity levels of specification prompts, although specific datasets and comparative benchmarks are not explicitly detailed in the summary. The results of the hazard analysis highlight pressing safety risks applicable to code synthesis LLMs, both at operational and systemic levels.

    5. Conclusions: The authors emphasize the importance of an ongoing evaluation of model capabilities and safety implications as part of the design and deployment processes for code synthesis LLMs. They outline critical hazards and mitigation strategies, advocating for a proactive approach in assessing these models as they evolve and become more integrated into technical workflows.
    The total cost is 0.00257925 USD

    Processing file: 4_4.77_Program synthesis with large language models.pdf
    Begin analyzing the article located at papers/papers_organized/4_4.77_Program synthesis with large language models.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: This paper investigates the capabilities of large language models in synthesizing Python programs from natural language descriptions, focusing on their performance in generating outputs for general-purpose programming tasks.

    2. Existing problems: Previous studies and datasets in program synthesis predominantly targeted domain-specific languages or utilized coding competition problems, which often obfuscate algorithms, leading to poor model performance. There has been a lack of focused benchmarking for general-purpose programming languages like Python that can assess model capabilities effectively.

    3. The main contributions: The paper introduces two novel datasets, the Mostly Basic Programming Problems (MBPP) and MathQA-Python, which contain structured and clearer programming tasks to facilitate better model evaluation. It finds that model performance improves significantly when fine-tuned on coding tasks and demonstrates the potential of integrating human feedback to enhance the generation of code.

    4. Experimental results: The evaluation encompassed multiple models with sizes ranging from 244M to 137B parameters on the MBPP and MathQA-Python datasets. Models achieved a synthesis rate of 59.6% on MBPP through few-shot learning and improved by about 10 percentage points with fine-tuning; the best model on MathQA-Python reached 83.8% accuracy. Compared to past benchmarks, these results indicate substantial progress in leveraging general language models for code synthesis.

    5. Conclusions: The findings indicate that large language models can successfully synthesize code from natural language descriptions, with performance scaling log-linearly with model size. The incorporation of human feedback significantly cuts error rates, suggesting a promising avenue for improving code generation tasks. Future research should focus on enhancing these models' capabilities to predict program outputs and further refining their performance across diverse programming challenges.
    The total cost is 0.00375225 USD

    Total cost for summarizing all files: 0.0063315

    The summaries for all selected files are printed below:
    ------Paper title: 9_0.195_A hazard analysis framework for code synthesis large language models.pdf------

    1. The main topic: The paper focuses on the safety and hazard analysis of Codex, a large language model (LLM) for code synthesis, including its capabilities and potential risks associated with its deployment in technical and non-technical contexts.

    2. Existing problems: Previous studies have primarily centered on simple code generation tasks and have not adequately addressed the alignment problems, misuse potential, and complex safety implications associated with advanced code synthesis models like Codex. Moreover, standard evaluation metrics have not considered the complexity of real-world coding scenarios or the potential hazards stemming from these models.

    3. The main contributions: The authors propose a novel evaluation framework to assess the generative capabilities of code synthesis LLMs against human ability and complexity of specification prompts. This framework is complemented by a hazard analysis tailored for LLMs, identifying risks associated with technical, social, political, and economic dimensions of deploying such models.

    4. Experimental results: The paper outlines qualitative metrics developed to benchmark the capabilities of Codex against complexity levels of specification prompts, although specific datasets and comparative benchmarks are not explicitly detailed in the summary. The results of the hazard analysis highlight pressing safety risks applicable to code synthesis LLMs, both at operational and systemic levels.

    5. Conclusions: The authors emphasize the importance of an ongoing evaluation of model capabilities and safety implications as part of the design and deployment processes for code synthesis LLMs. They outline critical hazards and mitigation strategies, advocating for a proactive approach in assessing these models as they evolve and become more integrated into technical workflows.



    ------Paper title: 4_4.77_Program synthesis with large language models.pdf------

    1. The main topic: This paper investigates the capabilities of large language models in synthesizing Python programs from natural language descriptions, focusing on their performance in generating outputs for general-purpose programming tasks.

    2. Existing problems: Previous studies and datasets in program synthesis predominantly targeted domain-specific languages or utilized coding competition problems, which often obfuscate algorithms, leading to poor model performance. There has been a lack of focused benchmarking for general-purpose programming languages like Python that can assess model capabilities effectively.

    3. The main contributions: The paper introduces two novel datasets, the Mostly Basic Programming Problems (MBPP) and MathQA-Python, which contain structured and clearer programming tasks to facilitate better model evaluation. It finds that model performance improves significantly when fine-tuned on coding tasks and demonstrates the potential of integrating human feedback to enhance the generation of code.

    4. Experimental results: The evaluation encompassed multiple models with sizes ranging from 244M to 137B parameters on the MBPP and MathQA-Python datasets. Models achieved a synthesis rate of 59.6% on MBPP through few-shot learning and improved by about 10 percentage points with fine-tuning; the best model on MathQA-Python reached 83.8% accuracy. Compared to past benchmarks, these results indicate substantial progress in leveraging general language models for code synthesis.

    5. Conclusions: The findings indicate that large language models can successfully synthesize code from natural language descriptions, with performance scaling log-linearly with model size. The incorporation of human feedback significantly cuts error rates, suggesting a promising avenue for improving code generation tasks. Future research should focus on enhancing these models' capabilities to predict program outputs and further refining their performance across diverse programming challenges.




    Would you like to check the code availability of the articles? (yes/no): 
    Checking code availability for the summarized articles...

    Checking code availability for: 9_0.195_A hazard analysis framework for code synthesis large language models.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    not available
    The total cost is 0.0021236999999999996 USD

    Checking code availability for: 4_4.77_Program synthesis with large language models.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    not available
    The total cost is 0.00475935 USD

    Total cost for checking code availability: 0.00688305 USD
    Total cost for the entire process (summaries + code availability check): 0.01321455 USD






|

.. code-block:: Python


    from auto_research.applications.surveys import topic_to_survey


    if __name__ == "__main__":
        """
        Main execution block for the `topic_to_survey` function.

        This block initializes the `topic_to_survey` function with the specified parameters and runs the automated research process.

        Example:
            # Sample usage:
            topic_to_survey(
                num_results=5,
                sort_by="relevance",
                date_cutoff="2024-12-01",
                score_threshold=0,
                destination_folder="papers",
                model="gpt-4o-mini",
                api_key_path="",
                api_key_type="OpenAI",
                organize_files=True,
                order_by_score=True,
                zip_folder=True,
                api_key=None,  # Directly provide the API key as a string. If None, the key will be retrieved from the file.
            )

        Parameters
        ----------
        num_results : int, optional
            Number of search results to retrieve. Defaults to 30.
        sort_by : str, optional
            Sorting criteria for search results. Options: "relevance", "date". Defaults to "relevance".
        date_cutoff : str, optional
            Cutoff date for search results. Only articles published before this date will be included. Defaults to "2024-12-01". Only relevant when `sort_by` is set as "date".
        score_threshold : float, optional
            Minimum score threshold for articles. Articles with a score below this will be excluded. Defaults to 0.5.
        destination_folder : str, optional
            Folder to store downloaded articles. Defaults to "papers".
        model : str, optional
            Model to use for summarization and keyword suggestions. Defaults to "gpt-4o-mini".
        api_key_path : str, optional
            Path to the directory containing the API key. Defaults to "../". Set it as "" if the file is located at the current directory.
        api_key_type : str, optional
            Type of API key to retrieve. Options: "OpenAI", "DeepSeek". Defaults to "OpenAI".
        organize_files : bool, optional
            Whether to organize the downloaded articles into subfolders based on their rank and score. Defaults to True.
        order_by_score : bool, optional
            Whether to order articles by their score when organizing. Defaults to True.
        zip_folder : bool, optional
            Whether to zip the organized folder after processing. Defaults to True.
        api_key : str, optional
            Directly provide the API key as a string. If None, the key will be retrieved from the file. Defaults to None.

        Returns
        -------
        None
        """
        topic_to_survey(
            num_results=5,
            sort_by="relevance",
            date_cutoff="2024-12-01",
            score_threshold=0,
            destination_folder="papers",
            model="gpt-4o-mini",
            api_key_path="",
            api_key_type="OpenAI",
            organize_files=True,
            order_by_score=True,
            zip_folder=True,
            api_key=None,
        )

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (4 minutes 42.881 seconds)


.. _sphx_glr_download__examples_gallery_top_to_survey.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: top_to_survey.ipynb <top_to_survey.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: top_to_survey.py <top_to_survey.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: top_to_survey.zip <top_to_survey.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
