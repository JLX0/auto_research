
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples_gallery/top_to_survey.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download__examples_gallery_top_to_survey.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_gallery_top_to_survey.py:


.. _top_to_survey_page:

Automatically converts a topic or question of interests into a survey over relevant papers
==========================================

When searching for research papers, the results from a search engine can vary significantly
depending on the specific keywords used, even if those keywords are conceptually similar.
For instance, searching for "LLMs" versus "Large Language Models" may yield different sets
of papers. Additionally, when experimenting with new keywords, it can be challenging to
remember whether a particular paper has already been checked. Furthermore, the process
of downloading papers and organizing them with appropriate filenames can be tedious and
time-consuming.

The function `topic_to_survey` streamlines the entire process by automating several key tasks.
It suggests multiple related keywords to ensure comprehensive coverage of the topic,
merges duplicate results to avoid redundancy, and automatically names downloaded files
using the paper titles for easy reference. Moreover, it leverages LLMs to generate summaries
of each paper, saving researchers valuable time and effort.

This script demonstrates the usage of the `topic_to_survey` function from the :mod:`auto_research.applications.surveys` module to:

- Conduct an automated research process based on a user-provided topic.
- Generate and refine a list of keywords for searching research articles.
- Retrieve and download articles based on the specified search criteria.
- Organize and summarize the downloaded articles.
- Check the code availability of the summarized articles (optional).

To get started with the package, you need to set up API keys. For detailed instructions, see :ref:`setting_up_api_keys`.

This script assumes that:

- A valid `key.json` file is available (located at the current working directory (""))

The process involves user interaction, including selecting keywords, summarizing articles, and optionally checking code availability.

Below is an example output from the following input:

- generate code with LLMs
- select
- 1,3
- select
- 2,3
- yes

.. GENERATED FROM PYTHON SOURCE LINES 48-119




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Please enter your research topic or question (e.g., 'Applications of AI in healthcare'): Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed

    Suggested keywords for searching articles based on your input:
    1. code generation with large language models
    2. code generation with LLM
    3. code synthesis using large language models
    4. automated code generation with large language models
    5. programmatic code generation with LLM
    6. large language models for programming
    7. large language models in software development
    8. AI-driven code generation
    9. machine learning for code generation
    10. natural language processing for programming
    11. neural networks for code synthesis
    12. text-to-code generation models
    13. language models for software engineering
    14. semantic code generation with deep learning
    15. domain-specific language generation with LLM

    How would you like to proceed with the suggested keywords?
    1. 'all': Use all the suggested keywords for searching.
    2. 'select': Choose specific keywords by their ranks.
    3. 'custom': Enter your own list of keywords manually.

    Choose an option ('all', 'select', or 'custom'): 
    Available keywords with their ranks:
    1. code generation with large language models
    2. code generation with LLM
    3. code synthesis using large language models
    4. automated code generation with large language models
    5. programmatic code generation with LLM
    6. large language models for programming
    7. large language models in software development
    8. AI-driven code generation
    9. machine learning for code generation
    10. natural language processing for programming
    11. neural networks for code synthesis
    12. text-to-code generation models
    13. language models for software engineering
    14. semantic code generation with deep learning
    15. domain-specific language generation with LLM

    Enter the ranks of the keywords you want to use, separated by commas (e.g., 1,3,5): 
    Using the following keywords: ['code generation with large language models', 'code synthesis using large language models']

    Final keywords to search: ['code generation with large language models', 'code synthesis using large language models']
    ------Searching for the 1th keyword 'code generation with large language models'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:05<00:21,  5.42s/it]    Searching papers:  40%|████      | 2/5 [00:09<00:13,  4.49s/it]    Searching papers:  60%|██████    | 3/5 [00:13<00:08,  4.20s/it]    Searching papers:  80%|████████  | 4/5 [00:17<00:04,  4.16s/it]    Searching papers: 100%|██████████| 5/5 [00:21<00:00,  4.30s/it]    Searching papers: 100%|██████████| 5/5 [00:21<00:00,  4.35s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 61.25312492028468
    Citation count: 693
    Year of publication: 2024
    Publication venue: Neural Information Processing Systems
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: Self-planning code generation with large language models
    Abstract:

    Large language models have demonstrated the ability to generate both natural
    language and programming language text. Such models open up the possibility of
    multi-language code generation: could code generation models generalize
    knowledge from one language to another? Although contemporary code generation
    models can generate semantically correct Python code, little is known about
    their abilities with other languages. We propose MultiPL-E, a system for
    translating unit test-driven code generation benchmarks to new languages. We
    create the first massively multilingual code generation benchmark by using
    MultiPL-E to translate two popular Python code generation benchmarks to 18
    additional programming languages.
      We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18
    languages that encompass a range of programming paradigms and popularity. Using
    these new parallel benchmarks, we evaluate the multi-language performance of
    three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We
    find that Codex matches or even exceeds its performance on Python for several
    other languages. The range of programming languages represented in MultiPL-E
    allow us to explore the impact of language frequency and language features on
    model performance. Finally, the MultiPL-E approach of compiling code generation
    benchmarks to new programming languages is both scalable and extensible, making
    it straightforward to evaluate new models, benchmarks, and languages.
    Combined Score: 10.78337841309485
    Citation count: 122
    Year of publication: 2024
    Publication venue: ACM Transactions on …
    Authors: X Jiang, Y Dong, L Wang, Z Fang, Q Shang


    Link: https://dl.acm.org/doi/pdf/10.1145/3672456
    ArXiv Link: http://arxiv.org/pdf/2208.08227v4
    Downloading Self-planning code generation with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Self-planning code generation with large language models.pdf.


    Paper 3:
    Title: Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models
    Abstract:

    Ensuring usability is crucial for the success of mobile apps. Usability
    issues can compromise user experience and negatively impact the perceived app
    quality. This paper presents UX-LLM, a novel tool powered by a Large
    Vision-Language Model that predicts usability issues in iOS apps. To evaluate
    the performance of UX-LLM we predicted usability issues in two open-source apps
    of a medium complexity and asked usability experts to assess the predictions.
    We also performed traditional usability testing and expert review for both apps
    and compared the results to those of UX-LLM. UX-LLM demonstrated precision
    ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its
    ability to identify valid usability issues, yet failing to capture the majority
    of issues. Finally, we conducted a focus group with an app development team of
    a capstone project developing a transit app for visually impaired persons. The
    focus group expressed positive perceptions of UX-LLM as it identified unknown
    usability issues in their app. However, they also raised concerns about its
    integration into the development workflow, suggesting potential improvements.
    Our results show that UX-LLM cannot fully replace traditional usability
    evaluation methods but serves as a valuable supplement particularly for small
    teams with limited resources, to identify issues in less common user paths, due
    to its ability to inspect the source code.
    Combined Score: 4.8515625
    Citation count: 621
    Year of publication: 2022
    Publication venue: Chi conference on human …
    Authors: P Vaithilingam, T Zhang, EL Glassman


    Link: https://dl.acm.org/doi/pdf/10.1145/3491101.3519665
    ArXiv Link: http://arxiv.org/pdf/2411.00634v1
    Downloading Expectation vs. experience Evaluating the usability of code generation tools powered by large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Expectation vs. experience Evaluating the usability of code generation tools powered by large language models.pdf.


    Paper 4:
    Title: Evaluating large language models in class-level code generation
    Abstract:

    Large language models have demonstrated the ability to generate both natural
    language and programming language text. Such models open up the possibility of
    multi-language code generation: could code generation models generalize
    knowledge from one language to another? Although contemporary code generation
    models can generate semantically correct Python code, little is known about
    their abilities with other languages. We propose MultiPL-E, a system for
    translating unit test-driven code generation benchmarks to new languages. We
    create the first massively multilingual code generation benchmark by using
    MultiPL-E to translate two popular Python code generation benchmarks to 18
    additional programming languages.
      We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18
    languages that encompass a range of programming paradigms and popularity. Using
    these new parallel benchmarks, we evaluate the multi-language performance of
    three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We
    find that Codex matches or even exceeds its performance on Python for several
    other languages. The range of programming languages represented in MultiPL-E
    allow us to explore the impact of language frequency and language features on
    model performance. Finally, the MultiPL-E approach of compiling code generation
    benchmarks to new programming languages is both scalable and extensible, making
    it straightforward to evaluate new models, benchmarks, and languages.
    Combined Score: 4.596194077712559
    Citation count: 52
    Year of publication: 2024
    Publication venue: International Conference on Software Engineering
    Authors: X Du, M Liu, K Wang, H Wang, J Liu, Y Chen


    Link: https://mingwei-liu.github.io/assets/pdf/ICSE2024ClassEval-V2.pdf
    ArXiv Link: http://arxiv.org/pdf/2208.08227v4
    Downloading Evaluating large language models in class-level code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Evaluating large language models in class-level code generation.pdf.


    Paper 5:
    Title: Planning with large language models for code generation
    Abstract:

    Developing domain models is one of the few remaining places that require
    manual human labor in AI planning. Thus, in order to make planning more
    accessible, it is desirable to automate the process of domain model generation.
    To this end, we investigate if large language models (LLMs) can be used to
    generate planning domain models from simple textual descriptions. Specifically,
    we introduce a framework for automated evaluation of LLM-generated domains by
    comparing the sets of plans for domain instances. Finally, we perform an
    empirical analysis of 7 large language models, including coding and chat models
    across 9 different planning domains, and under three classes of natural
    language domain descriptions. Our results indicate that LLMs, particularly
    those with high parameter counts, exhibit a moderate level of proficiency in
    generating correct planning domains from natural language descriptions. Our
    code is available at https://github.com/IBM/NL2PDDL.
    Combined Score: 2.9081346892514484
    Citation count: 136
    Year of publication: 2023
    Publication venue: arXiv preprint arXiv …
    Authors: S Zhang, Z Chen, Y Shen, M Ding


    Link: https://arxiv.org/pdf/2303.05510
    ArXiv Link: http://arxiv.org/pdf/2405.06650v1
    Downloading Planning with large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Planning with large language models for code generation.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    ------Searching for the 2th keyword 'code synthesis using large language models'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:05<00:22,  5.58s/it]    Searching papers:  40%|████      | 2/5 [00:08<00:12,  4.01s/it]    Searching papers:  60%|██████    | 3/5 [00:11<00:06,  3.47s/it]    Searching papers:  80%|████████  | 4/5 [00:15<00:03,  3.67s/it]    Searching papers: 100%|██████████| 5/5 [00:18<00:00,  3.62s/it]    Searching papers: 100%|██████████| 5/5 [00:18<00:00,  3.77s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 61.25312492028468
    Citation count: 693
    Year of publication: 2024
    Publication venue: Advances in Neural …
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: Evaluating large language models trained on code
    Abstract:

    We focus on the problem of language modeling for code-switched language, in
    the context of automatic speech recognition (ASR). Language modeling for
    code-switched language is challenging for (at least) three reasons: (1) lack of
    available large-scale code-switched data for training; (2) lack of a replicable
    evaluation setup that is ASR directed yet isolates language modeling
    performance from the other intricacies of the ASR system; and (3) the reliance
    on generative modeling. We tackle these three issues: we propose an
    ASR-motivated evaluation setup which is decoupled from an ASR system and the
    choice of vocabulary, and provide an evaluation dataset for English-Spanish
    code-switching. This setup lends itself to a discriminative training approach,
    which we demonstrate to work better than generative language modeling. Finally,
    we explore a variety of training protocols and verify the effectiveness of
    training with large amounts of monolingual data followed by fine-tuning with
    small amounts of code-switched data, for both the generative and discriminative
    cases.
    Combined Score: 13.266144096910752
    Citation count: 3708
    Year of publication: 2021
    Publication venue: arXiv.org
    Authors: M Chen, J Tworek, H Jun, Q Yuan, HPDO Pinto


    Link: https://arxiv.org/pdf/2107.03374.pdf?spm=a2c6h.13046898.publish-article.19.6cd56ffaIPu4NQ&file=2107.03374
    ArXiv Link: http://arxiv.org/pdf/1810.11895v3
    Downloading Evaluating large language models trained on code.pdf... with upper time limit: 10 seconds
    Downloaded: Evaluating large language models trained on code.pdf.


    Paper 3:
    Title: A systematic evaluation of large language models of code
    Abstract:

    Language models (LMs) have exhibited impressive abilities in generating codes
    from natural language requirements. In this work, we highlight the diversity of
    code generated by LMs as a critical criterion for evaluating their code
    generation capabilities, in addition to functional correctness. Despite its
    practical implications, there is a lack of studies focused on assessing the
    diversity of generated code, which overlooks its importance in the development
    of code LMs. We propose a systematic approach to evaluate the diversity of
    generated code, utilizing various metrics for inter-code similarity as well as
    functional correctness. Specifically, we introduce a pairwise code similarity
    measure that leverages large LMs' capabilities in code understanding and
    reasoning, demonstrating the highest correlation with human judgment. We
    extensively investigate the impact of various factors on the quality of
    generated code, including model sizes, temperatures, training approaches,
    prompting strategies, and the difficulty of input problems. Our consistent
    observation of a positive correlation between the test pass score and the
    inter-code similarity score indicates that current LMs tend to produce
    functionally correct code with limited diversity.
    Combined Score: 5.265625
    Citation count: 674
    Year of publication: 2022
    Publication venue: MAPS@PLDI
    Authors: FF Xu, U Alon, G Neubig, VJ Hellendoorn


    Link: https://dl.acm.org/doi/pdf/10.1145/3520312.3534862
    ArXiv Link: http://arxiv.org/pdf/2408.14504v1
    Downloading A systematic evaluation of large language models of code.pdf... with upper time limit: 10 seconds
    Downloaded: A systematic evaluation of large language models of code.pdf.


    Paper 4:
    Title: Program synthesis with large language models
    Abstract:

    GitHub Copilot, an extension for the Visual Studio Code development
    environment powered by the large-scale language model Codex, makes automatic
    program synthesis available for software developers. This model has been
    extensively studied in the field of deep learning, however, a comparison to
    genetic programming, which is also known for its performance in automatic
    program synthesis, has not yet been carried out. In this paper, we evaluate
    GitHub Copilot on standard program synthesis benchmark problems and compare the
    achieved results with those from the genetic programming literature. In
    addition, we discuss the performance of both approaches. We find that the
    performance of the two approaches on the benchmark problems is quite similar,
    however, in comparison to GitHub Copilot, the program synthesis approaches
    based on genetic programming are not yet mature enough to support programmers
    in practical software development. Genetic programming usually needs a huge
    amount of expensive hand-labeled training cases and takes too much time to
    generate solutions. Furthermore, source code generated by genetic programming
    approaches is often bloated and difficult to understand. For future work on
    program synthesis with genetic programming, we suggest researchers to focus on
    improving the execution time, readability, and usability.
    Combined Score: 4.8084405788155475
    Citation count: 1344
    Year of publication: 2021
    Publication venue: arXiv preprint arXiv …
    Authors: J Austin, A Odena, M Nye, M Bosma


    Link: https://arxiv.org/pdf/2108.07732
    ArXiv Link: http://arxiv.org/pdf/2111.07875v1
    Downloading Program synthesis with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Program synthesis with large language models.pdf.


    Paper 5:
    Title: Jigsaw: Large language models meet program synthesis
    Abstract:

    Large pre-trained language models such as GPT-3, Codex, and Google's language
    model are now capable of generating code from natural language specifications
    of programmer intent. We view these developments with a mixture of optimism and
    caution. On the optimistic side, such large language models have the potential
    to improve productivity by providing an automated AI pair programmer for every
    programmer in the world. On the cautionary side, since these large language
    models do not understand program semantics, they offer no guarantees about
    quality of the suggested code. In this paper, we present an approach to augment
    these large language models with post-processing steps based on program
    analysis and synthesis techniques, that understand the syntax and semantics of
    programs. Further, we show that such techniques can make use of user feedback
    and improve with usage. We present our experiences from building and evaluating
    such a tool jigsaw, targeted at synthesizing code for using Python Pandas API
    using multi-modal inputs. Our experience suggests that as these large language
    models evolve for synthesizing code from intent, jigsaw has an important role
    to play in improving the accuracy of the systems.
    Combined Score: 1.7421875
    Citation count: 223
    Year of publication: 2022
    Publication venue: International Conference on Software Engineering
    Authors: N Jain, S Vaidyanath, A Iyer, N Natarajan


    Link: https://arxiv.org/pdf/2112.02969
    ArXiv Link: http://arxiv.org/pdf/2112.02969v1
    Downloading Jigsaw Large language models meet program synthesis.pdf... with upper time limit: 10 seconds
    Downloaded: Jigsaw Large language models meet program synthesis.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    Files organized in papers/papers_organized
    Target folder saved to papers/papers_organized.zip
    The entire source folder saved to papers.zip

    How would you like to summarize the papers?
    1. 'all': Summarize all papers in the organized folder.
    2. 'select': Choose specific papers by their ranks to summarize.

    Choose an option ('all' or 'select'): 
    Available papers with their ranks:
    1. 1_61.3_Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf
    2. 2_13.3_Evaluating large language models trained on code.pdf
    3. 3_10.8_Self-planning code generation with large language models.pdf
    4. 4_5.27_A systematic evaluation of large language models of code.pdf
    5. 5_4.85_Expectation vs. experience Evaluating the usability of code generation tools powered by large language models.pdf
    6. 6_4.81_Program synthesis with large language models.pdf
    7. 7_4.6_Evaluating large language models in class-level code generation.pdf
    8. 8_2.91_Planning with large language models for code generation.pdf
    9. 9_1.74_Jigsaw Large language models meet program synthesis.pdf

    Enter the ranks of the papers you want to summarize, separated by commas (e.g., 1,3,5): 
    Summarizing the following papers: ['2_13.3_Evaluating large language models trained on code.pdf', '3_10.8_Self-planning code generation with large language models.pdf']

    Processing file: 2_13.3_Evaluating large language models trained on code.pdf
    Begin analyzing the article located at papers/papers_organized/2_13.3_Evaluating large language models trained on code.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper presents Codex, a GPT language model specifically fine-tuned on publicly available code from GitHub, focusing on its capabilities for synthesizing Python code from natural language docstrings.

    2. Existing problems: Previous models, such as GPT-3, struggled with generating correct code from natural language descriptions, achieving a 0% success rate on functional correctness in coding tasks. There are challenges related to evaluating the correctness of program outputs and understanding the broader implications of deploying code generation technology.

    3. The main contributions: The paper demonstrates Codex's improved performance over existing models on the HumanEval benchmark, with a 28.8% success rate compared to 0% for GPT-3 and 11.4% for GPT-J. Additionally, it introduces the concept of repeated sampling from the model to enhance the likelihood of generating correct solutions, showcasing significant advancements in program synthesis capabilities.

    4. Experimental results: Codex was evaluated using the HumanEval dataset, which comprises problems designed to measure functional correctness for code synthesis. By employing repeated sampling, the model achieved a 70.2% success rate across 100 samples per problem, indicating substantial improvements over prior models.

    5. Conclusions: The study concludes that large language models like Codex can effectively generate functionally correct code from natural language instructions, though there are notable limitations such as difficulties with complex descriptions and variable binding. The paper emphasizes the importance of careful deployment and continued exploration of the societal impacts of code generation technologies. Future directions include addressing model limitations and enhancing training methodologies for better performance.
    The total cost is 0.0028549499999999998 USD

    Processing file: 3_10.8_Self-planning code generation with large language models.pdf
    Begin analyzing the article located at papers/papers_organized/3_10.8_Self-planning code generation with large language models.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper introduces a self-planning code generation approach leveraging large language models (LLMs) to improve the handling of complex human intent in code generation tasks.

    2. Existing problems: Previous code generation methods struggled with complex and nuanced human-intended tasks, often leading to insufficient planning and poor code quality. Directly generating code from intent without a structured planning phase proved to be a major limitation.

    3. The main contributions: This study proposes a two-phase self-planning approach where LLMs first generate concise solution steps before implementing the code. This method benefits from few-shot prompting to facilitate the planning process without requiring extensive training on intent-plan pairs.

    4. Experimental results: The self-planning approach demonstrated significant performance improvements in code generation, achieving up to 25.4% improvement in Pass@1 scores compared to direct generation and up to 11.9% compared to chain-of-thought approaches. The evaluation was conducted across various programming languages, including Python, Java, Go, and JavaScript.

    5. Conclusions: The findings highlight that self-planning can substantially enhance the code generation capabilities of LLMs by improving correctness, readability, and robustness of the generated code. This work opens avenues for further exploration into planning methods in code generation for more complex programming challenges.
    The total cost is 0.0021886499999999995 USD

    Total cost for summarizing all files: 0.005043599999999999

    The summaries for all selected files are printed below:
    ------Paper title: 2_13.3_Evaluating large language models trained on code.pdf------

    1. The main topic: The paper presents Codex, a GPT language model specifically fine-tuned on publicly available code from GitHub, focusing on its capabilities for synthesizing Python code from natural language docstrings.

    2. Existing problems: Previous models, such as GPT-3, struggled with generating correct code from natural language descriptions, achieving a 0% success rate on functional correctness in coding tasks. There are challenges related to evaluating the correctness of program outputs and understanding the broader implications of deploying code generation technology.

    3. The main contributions: The paper demonstrates Codex's improved performance over existing models on the HumanEval benchmark, with a 28.8% success rate compared to 0% for GPT-3 and 11.4% for GPT-J. Additionally, it introduces the concept of repeated sampling from the model to enhance the likelihood of generating correct solutions, showcasing significant advancements in program synthesis capabilities.

    4. Experimental results: Codex was evaluated using the HumanEval dataset, which comprises problems designed to measure functional correctness for code synthesis. By employing repeated sampling, the model achieved a 70.2% success rate across 100 samples per problem, indicating substantial improvements over prior models.

    5. Conclusions: The study concludes that large language models like Codex can effectively generate functionally correct code from natural language instructions, though there are notable limitations such as difficulties with complex descriptions and variable binding. The paper emphasizes the importance of careful deployment and continued exploration of the societal impacts of code generation technologies. Future directions include addressing model limitations and enhancing training methodologies for better performance.



    ------Paper title: 3_10.8_Self-planning code generation with large language models.pdf------

    1. The main topic: The paper introduces a self-planning code generation approach leveraging large language models (LLMs) to improve the handling of complex human intent in code generation tasks.

    2. Existing problems: Previous code generation methods struggled with complex and nuanced human-intended tasks, often leading to insufficient planning and poor code quality. Directly generating code from intent without a structured planning phase proved to be a major limitation.

    3. The main contributions: This study proposes a two-phase self-planning approach where LLMs first generate concise solution steps before implementing the code. This method benefits from few-shot prompting to facilitate the planning process without requiring extensive training on intent-plan pairs.

    4. Experimental results: The self-planning approach demonstrated significant performance improvements in code generation, achieving up to 25.4% improvement in Pass@1 scores compared to direct generation and up to 11.9% compared to chain-of-thought approaches. The evaluation was conducted across various programming languages, including Python, Java, Go, and JavaScript.

    5. Conclusions: The findings highlight that self-planning can substantially enhance the code generation capabilities of LLMs by improving correctness, readability, and robustness of the generated code. This work opens avenues for further exploration into planning methods in code generation for more complex programming challenges.



    Would you like to check the code availability of the articles? (yes/no): 
    Checking code availability for the summarized articles...

    Checking code availability for: 2_13.3_Evaluating large language models trained on code.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    https://www.github.com/openai/human-eval
    The total cost is 0.00602715 USD

    Checking code availability for: 3_10.8_Self-planning code generation with large language models.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    not available
    The total cost is 0.0038326499999999995 USD

    Total cost for checking code availability: 0.0098598 USD
    Total cost for the entire process (summaries + code availability check): 0.014903399999999999 USD






|

.. code-block:: Python


    from auto_research.applications.surveys import topic_to_survey


    if __name__ == "__main__":
        """
        Main execution block for the `topic_to_survey` function.

        This block initializes the `topic_to_survey` function with the specified parameters and runs the automated research process.

        Example:
            # Sample usage:
            topic_to_survey(
                num_results=5,
                sort_by="relevance",
                date_cutoff="2024-12-01",
                score_threshold=0,
                destination_folder="papers",
                model="gpt-4o-mini",
                api_key_path="",
                api_key_type="OpenAI",
                organize_files=True,
                order_by_score=True,
                zip_folder=True,
                api_key=None,  # Directly provide the API key as a string. If None, the key will be retrieved from the file.
            )

        Parameters
        ----------
        num_results : int, optional
            Number of search results to retrieve. Defaults to 30.
        sort_by : str, optional
            Sorting criteria for search results. Options: "relevance", "date". Defaults to "relevance".
        date_cutoff : str, optional
            Cutoff date for search results. Only articles published before this date will be included. Defaults to "2024-12-01". Only relevant when `sort_by` is set as "date".
        score_threshold : float, optional
            Minimum score threshold for articles. Articles with a score below this will be excluded. Defaults to 0.5.
        destination_folder : str, optional
            Folder to store downloaded articles. Defaults to "papers".
        model : str, optional
            Model to use for summarization and keyword suggestions. Defaults to "gpt-4o-mini".
        api_key_path : str, optional
            Path to the directory containing the API key. Defaults to "../". Set it as "" if the file is located at the current directory.
        api_key_type : str, optional
            Type of API key to retrieve. Options: "OpenAI", "DeepSeek". Defaults to "OpenAI".
        organize_files : bool, optional
            Whether to organize the downloaded articles into subfolders based on their rank and score. Defaults to True.
        order_by_score : bool, optional
            Whether to order articles by their score when organizing. Defaults to True.
        zip_folder : bool, optional
            Whether to zip the organized folder after processing. Defaults to True.
        api_key : str, optional
            Directly provide the API key as a string. If None, the key will be retrieved from the file. Defaults to None.

        Returns
        -------
        None
        """
        topic_to_survey(
            num_results=5,
            sort_by="relevance",
            date_cutoff="2024-12-01",
            score_threshold=0,
            destination_folder="papers",
            model="gpt-4o-mini",
            api_key_path="",
            api_key_type="OpenAI",
            organize_files=True,
            order_by_score=True,
            zip_folder=True,
            api_key=None,
        )

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (4 minutes 37.687 seconds)


.. _sphx_glr_download__examples_gallery_top_to_survey.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: top_to_survey.ipynb <top_to_survey.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: top_to_survey.py <top_to_survey.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: top_to_survey.zip <top_to_survey.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
