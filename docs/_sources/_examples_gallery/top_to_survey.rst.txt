
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "_examples_gallery/top_to_survey.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download__examples_gallery_top_to_survey.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr__examples_gallery_top_to_survey.py:


.. _top_to_survey_page:

Automatically converts a topic or question of interests into a survey over relevant papers
==========================================

When searching for research papers, the results from a search engine can vary significantly
depending on the specific keywords used, even if those keywords are conceptually similar.
For instance, searching for "LLMs" versus "Large Language Models" may yield different sets
of papers. Additionally, when experimenting with new keywords, it can be challenging to
remember whether a particular paper has already been checked. Furthermore, the process
of downloading papers and organizing them with appropriate filenames can be tedious and
time-consuming.

The function `topic_to_survey` streamlines the entire process by automating several key tasks.
It suggests multiple related keywords to ensure comprehensive coverage of the topic,
merges duplicate results to avoid redundancy, automatically names downloaded files
using the paper titles for easy reference, and automatically ranks the papers based on their impacts
(see :mod:`auto_research.search.core.AutoSearch.score_threshold`). Moreover, it leverages LLMs
to generate summaries of each paper, saving researchers valuable time and effort.

This script demonstrates the usage of the `topic_to_survey` function from the :mod:`auto_research.applications.surveys` module to:

- Conduct an automated research process based on a user-provided topic.
- Generate and refine a list of keywords for searching research articles.
- Retrieve and download articles based on the specified search criteria.
- Rank, organize, and summarize the downloaded articles.
- Check the code availability of the summarized articles (optional).

To get started with the package, you need to set up API keys. For detailed instructions, see :ref:`setting_up_api_keys`.

This script assumes that:

- A valid `key.json` file is available (located at the current working directory (""))

The process involves user interaction, including selecting keywords, summarizing articles, and optionally checking code availability.

Below is an example output from the following input:

- generate code with LLMs
- select
- 1,3
- select
- 2,3
- yes

.. GENERATED FROM PYTHON SOURCE LINES 49-120




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Please enter your research topic or question (e.g., 'Applications of AI in healthcare'): Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed

    Suggested keywords for searching articles based on your input:
    1. code generation with language models
    2. code generation with large language models
    3. code synthesis using language models
    4. automatic code generation
    5. program synthesis with large language models
    6. machine learning for code generation
    7. deep learning for code generation
    8. code completion using language models
    9. program generation with LLMs
    10. AI-assisted code generation

    How would you like to proceed with the suggested keywords?
    1. 'all': Use all the suggested keywords for searching.
    2. 'select': Choose specific keywords by their ranks.
    3. 'custom': Enter your own list of keywords manually.

    Choose an option ('all', 'select', or 'custom'): 
    Available keywords with their ranks:
    1. code generation with language models
    2. code generation with large language models
    3. code synthesis using language models
    4. automatic code generation
    5. program synthesis with large language models
    6. machine learning for code generation
    7. deep learning for code generation
    8. code completion using language models
    9. program generation with LLMs
    10. AI-assisted code generation

    Enter the ranks of the keywords you want to use, separated by commas (e.g., 1,3,5): 
    Using the following keywords: ['code generation with language models', 'code synthesis using language models']

    Final keywords to search: ['code generation with language models', 'code synthesis using language models']
    ------Searching for the 1th keyword 'code generation with language models'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:06<00:25,  6.44s/it]    Searching papers:  40%|████      | 2/5 [00:09<00:13,  4.64s/it]    Searching papers:  60%|██████    | 3/5 [00:15<00:10,  5.06s/it]    Searching papers:  80%|████████  | 4/5 [00:19<00:04,  4.52s/it]    Searching papers: 100%|██████████| 5/5 [00:24<00:00,  4.72s/it]    Searching papers: 100%|██████████| 5/5 [00:24<00:00,  4.83s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 16.636241089982548
    Citation count: 778
    Year of publication: 2023
    Publication venue: Advances in Neural …
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: Self-planning code generation with large language models
    Abstract:

    Large language models have demonstrated the ability to generate both natural
    language and programming language text. Such models open up the possibility of
    multi-language code generation: could code generation models generalize
    knowledge from one language to another? Although contemporary code generation
    models can generate semantically correct Python code, little is known about
    their abilities with other languages. We propose MultiPL-E, a system for
    translating unit test-driven code generation benchmarks to new languages. We
    create the first massively multilingual code generation benchmark by using
    MultiPL-E to translate two popular Python code generation benchmarks to 18
    additional programming languages.
      We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18
    languages that encompass a range of programming paradigms and popularity. Using
    these new parallel benchmarks, we evaluate the multi-language performance of
    three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We
    find that Codex matches or even exceeds its performance on Python for several
    other languages. The range of programming languages represented in MultiPL-E
    allow us to explore the impact of language frequency and language features on
    model performance. Finally, the MultiPL-E approach of compiling code generation
    benchmarks to new programming languages is both scalable and extensible, making
    it straightforward to evaluate new models, benchmarks, and languages.
    Combined Score: 11.932426932522988
    Citation count: 135
    Year of publication: 2024
    Publication venue: ACM Transactions on …
    Authors: X Jiang, Y Dong, L Wang, Z Fang, Q Shang


    Link: https://dl.acm.org/doi/full/10.1145/3672456
    ArXiv Link: http://arxiv.org/pdf/2208.08227v4
    Downloading Self-planning code generation with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Self-planning code generation with large language models.pdf.
    /home/j/experiments/auto_research/auto_research/search/files_management.py:55: UserWarning: Error opening PDF: Failed to open file 'papers/Self-planning code generation with large language models.pdf'.
      warnings.warn(f"Error opening PDF: {e}", UserWarning)
    The downloaded PDF file 'Self-planning code generation with large language models.pdf' is corrupted.
    File removed: Self-planning code generation with large language models.pdf
    Trying to download from ArXiv link: http://arxiv.org/pdf/2208.08227v4
    Downloading Self-planning code generation with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Self-planning code generation with large language models.pdf.


    Paper 3:
    Title: A survey on large language models for code generation
    Abstract:

    Large Language Models (LLMs) have garnered remarkable advancements across
    diverse code-related tasks, known as Code LLMs, particularly in code generation
    that generates source code with LLM from natural language descriptions. This
    burgeoning field has captured significant interest from both academic
    researchers and industry professionals due to its practical significance in
    software development, e.g., GitHub Copilot. Despite the active exploration of
    LLMs for a variety of code tasks, either from the perspective of natural
    language processing (NLP) or software engineering (SE) or both, there is a
    noticeable absence of a comprehensive and up-to-date literature review
    dedicated to LLM for code generation. In this survey, we aim to bridge this gap
    by providing a systematic literature review that serves as a valuable reference
    for researchers investigating the cutting-edge progress in LLMs for code
    generation. We introduce a taxonomy to categorize and discuss the recent
    developments in LLMs for code generation, covering aspects such as data
    curation, latest advances, performance evaluation, ethical implications,
    environmental impact, and real-world applications. In addition, we present a
    historical overview of the evolution of LLMs for code generation and offer an
    empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks
    across various levels of difficulty and types of programming tasks to highlight
    the progressive enhancements in LLM capabilities for code generation. We
    identify critical challenges and promising opportunities regarding the gap
    between academia and practical development. Furthermore, we have established a
    dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey)
    to continuously document and disseminate the most recent advances in the field.
    Combined Score: 10.16465997955662
    Citation count: 115
    Year of publication: 2024
    Publication venue: arXiv preprint arXiv:2406.00515
    Authors: J Jiang, F Wang, J Shen, S Kim, S Kim


    Link: https://arxiv.org/pdf/2406.00515
    ArXiv Link: http://arxiv.org/pdf/2406.00515v2
    Downloading A survey on large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: A survey on large language models for code generation.pdf.


    Paper 4:
    Title: Planning with large language models for code generation
    Abstract:

    Developing domain models is one of the few remaining places that require
    manual human labor in AI planning. Thus, in order to make planning more
    accessible, it is desirable to automate the process of domain model generation.
    To this end, we investigate if large language models (LLMs) can be used to
    generate planning domain models from simple textual descriptions. Specifically,
    we introduce a framework for automated evaluation of LLM-generated domains by
    comparing the sets of plans for domain instances. Finally, we perform an
    empirical analysis of 7 large language models, including coding and chat models
    across 9 different planning domains, and under three classes of natural
    language domain descriptions. Our results indicate that LLMs, particularly
    those with high parameter counts, exhibit a moderate level of proficiency in
    generating correct planning domains from natural language descriptions. Our
    code is available at https://github.com/IBM/NL2PDDL.
    Combined Score: 3.2930348687111985
    Citation count: 154
    Year of publication: 2023
    Publication venue: arXiv preprint arXiv …
    Authors: S Zhang, Z Chen, Y Shen, M Ding


    Link: https://arxiv.org/pdf/2303.05510
    ArXiv Link: http://arxiv.org/pdf/2405.06650v1
    Downloading Planning with large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Planning with large language models for code generation.pdf.


    Paper 5:
    Title: A survey on evaluating large language models in code generation tasks
    Abstract:

    This paper provides a comprehensive review of the current methods and metrics
    used to evaluate the performance of Large Language Models (LLMs) in code
    generation tasks. With the rapid growth in demand for automated software
    development, LLMs have demonstrated significant potential in the field of code
    generation. The paper begins by reviewing the historical development of LLMs
    and their applications in code generation. Next, it details various methods and
    metrics for assessing the code generation capabilities of LLMs, including code
    correctness, efficiency, readability, and evaluation methods based on expert
    review and user experience. The paper also evaluates the widely used benchmark
    datasets, identifying their limitations and proposing directions for future
    improvements. Specifically, the paper analyzes the performance of code
    generation models across different tasks by combining multiple evaluation
    metrics, such as code compilation/interpretation success rates, unit test pass
    rates, and performance and efficiency metrics, to comprehensively assess the
    practical application of LLMs in code generation. Finally, the paper discusses
    the challenges faced in evaluating LLMs in code generation, particularly how to
    ensure the comprehensiveness and accuracy of evaluation methods and how to
    adapt to the evolving practices of software development. These analyses and
    discussions provide valuable insights for further optimizing and improving the
    application of LLMs in code generation tasks.
    Combined Score: 0.795495128834866
    Citation count: 9
    Year of publication: 2024
    Publication venue: arXiv.org
    Authors: L Chen, Q Guo, H Jia, Z Zeng, X Wang, Y Xu


    Link: https://arxiv.org/pdf/2408.16498
    ArXiv Link: http://arxiv.org/pdf/2408.16498v1
    Downloading A survey on evaluating large language models in code generation tasks.pdf... with upper time limit: 10 seconds
    Downloaded: A survey on evaluating large language models in code generation tasks.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip
    ------Searching for the 2th keyword 'code synthesis using language models'------
    Searching papers:   0%|          | 0/5 [00:00<?, ?it/s]    Searching papers:  20%|██        | 1/5 [00:23<01:34, 23.58s/it]    Searching papers:  40%|████      | 2/5 [00:27<00:36, 12.25s/it]    Searching papers:  60%|██████    | 3/5 [00:30<00:15,  7.95s/it]    Searching papers:  80%|████████  | 4/5 [00:37<00:07,  7.58s/it]    Searching papers: 100%|██████████| 5/5 [00:41<00:00,  6.26s/it]    Searching papers: 100%|██████████| 5/5 [00:41<00:00,  8.34s/it]


    Paper 1:
    Title: Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation
    Abstract:

    Program synthesis has been long studied with recent approaches focused on
    directly using the power of Large Language Models (LLMs) to generate code.
    Programming benchmarks, with curated synthesis problems and test-cases, are
    used to measure the performance of various LLMs on code synthesis. However,
    these test-cases can be limited in both quantity and quality for fully
    assessing the functional correctness of the generated code. Such limitation in
    the existing benchmarks begs the following question: In the era of LLMs, is the
    code generated really correct? To answer this, we propose EvalPlus -- a code
    synthesis evaluation framework to rigorously benchmark the functional
    correctness of LLM-synthesized code. EvalPlus augments a given evaluation
    dataset with large amounts of test-cases newly produced by an automatic test
    input generator, powered by both LLM- and mutation-based strategies. While
    EvalPlus is general, we extend the test-cases of the popular HumanEval
    benchmark by 80x to build HumanEval+. Our extensive evaluation across 26
    popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to
    catch significant amounts of previously undetected wrong code synthesized by
    LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that
    test insufficiency can lead to mis-ranking. For example, both
    WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,
    while none of them could on HumanEval. Our work not only indicates that prior
    popular code synthesis evaluation results do not accurately reflect the true
    performance of LLMs for code synthesis, but also opens up a new direction to
    improve such programming benchmarks through automated testing. We have
    open-sourced our tools, enhanced datasets as well as all LLM-generated code at
    https://github.com/evalplus/evalplus to facilitate and accelerate future
    LLM-for-code research.
    Combined Score: 16.636241089982548
    Citation count: 778
    Year of publication: 2023
    Publication venue: Advances in Neural …
    Authors: J Liu, CS Xia, Y Wang, L Zhang


    Link: https://proceedings.neurips.cc/paper_files/paper/2023/file/43e9d647ccd3e4b7b5baab53f0368686-Paper-Conference.pdf
    ArXiv Link: http://arxiv.org/pdf/2305.01210v3
    Downloading Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf... with upper time limit: 10 seconds
    Downloaded: Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf.


    Paper 2:
    Title: A systematic evaluation of large language models of code
    Abstract:

    Language models (LMs) have exhibited impressive abilities in generating codes
    from natural language requirements. In this work, we highlight the diversity of
    code generated by LMs as a critical criterion for evaluating their code
    generation capabilities, in addition to functional correctness. Despite its
    practical implications, there is a lack of studies focused on assessing the
    diversity of generated code, which overlooks its importance in the development
    of code LMs. We propose a systematic approach to evaluate the diversity of
    generated code, utilizing various metrics for inter-code similarity as well as
    functional correctness. Specifically, we introduce a pairwise code similarity
    measure that leverages large LMs' capabilities in code understanding and
    reasoning, demonstrating the highest correlation with human judgment. We
    extensively investigate the impact of various factors on the quality of
    generated code, including model sizes, temperatures, training approaches,
    prompting strategies, and the difficulty of input problems. Our consistent
    observation of a positive correlation between the test pass score and the
    inter-code similarity score indicates that current LMs tend to produce
    functionally correct code with limited diversity.
    Combined Score: 5.3984375
    Citation count: 691
    Year of publication: 2022
    Publication venue: Proceedings of the 6th ACM …
    Authors: FF Xu, U Alon, G Neubig, VJ Hellendoorn


    Link: https://dl.acm.org/doi/pdf/10.1145/3520312.3534862
    ArXiv Link: http://arxiv.org/pdf/2408.14504v1
    Downloading A systematic evaluation of large language models of code.pdf... with upper time limit: 10 seconds
    Downloaded: A systematic evaluation of large language models of code.pdf.


    Paper 3:
    Title: Program synthesis with large language models
    Abstract:

    GitHub Copilot, an extension for the Visual Studio Code development
    environment powered by the large-scale language model Codex, makes automatic
    program synthesis available for software developers. This model has been
    extensively studied in the field of deep learning, however, a comparison to
    genetic programming, which is also known for its performance in automatic
    program synthesis, has not yet been carried out. In this paper, we evaluate
    GitHub Copilot on standard program synthesis benchmark problems and compare the
    achieved results with those from the genetic programming literature. In
    addition, we discuss the performance of both approaches. We find that the
    performance of the two approaches on the benchmark problems is quite similar,
    however, in comparison to GitHub Copilot, the program synthesis approaches
    based on genetic programming are not yet mature enough to support programmers
    in practical software development. Genetic programming usually needs a huge
    amount of expensive hand-labeled training cases and takes too much time to
    generate solutions. Furthermore, source code generated by genetic programming
    approaches is often bloated and difficult to understand. For future work on
    program synthesis with genetic programming, we suggest researchers to focus on
    improving the execution time, readability, and usability.
    Combined Score: 5.219877086675509
    Citation count: 1459
    Year of publication: 2021
    Publication venue: arXiv.org
    Authors: J Austin, A Odena, M Nye, M Bosma


    Link: https://arxiv.org/pdf/2108.07732
    ArXiv Link: http://arxiv.org/pdf/2111.07875v1
    Downloading Program synthesis with large language models.pdf... with upper time limit: 10 seconds
    Downloaded: Program synthesis with large language models.pdf.


    Paper 4:
    Title: Jigsaw: Large language models meet program synthesis
    Abstract:

    Large pre-trained language models such as GPT-3, Codex, and Google's language
    model are now capable of generating code from natural language specifications
    of programmer intent. We view these developments with a mixture of optimism and
    caution. On the optimistic side, such large language models have the potential
    to improve productivity by providing an automated AI pair programmer for every
    programmer in the world. On the cautionary side, since these large language
    models do not understand program semantics, they offer no guarantees about
    quality of the suggested code. In this paper, we present an approach to augment
    these large language models with post-processing steps based on program
    analysis and synthesis techniques, that understand the syntax and semantics of
    programs. Further, we show that such techniques can make use of user feedback
    and improve with usage. We present our experiences from building and evaluating
    such a tool jigsaw, targeted at synthesizing code for using Python Pandas API
    using multi-modal inputs. Our experience suggests that as these large language
    models evolve for synthesizing code from intent, jigsaw has an important role
    to play in improving the accuracy of the systems.
    Combined Score: 1.7890625
    Citation count: 229
    Year of publication: 2022
    Publication venue: International Conference on Software Engineering
    Authors: N Jain, S Vaidyanath, A Iyer, N Natarajan


    Link: https://arxiv.org/pdf/2112.02969
    ArXiv Link: http://arxiv.org/pdf/2112.02969v1
    Downloading Jigsaw Large language models meet program synthesis.pdf... with upper time limit: 10 seconds
    Downloaded: Jigsaw Large language models meet program synthesis.pdf.


    Paper 5:
    Title: A hazard analysis framework for code synthesis large language models
    Abstract:

    Codex, a large language model (LLM) trained on a variety of codebases,
    exceeds the previous state of the art in its capacity to synthesize and
    generate code. Although Codex provides a plethora of benefits, models that may
    generate code on such scale have significant limitations, alignment problems,
    the potential to be misused, and the possibility to increase the rate of
    progress in technical fields that may themselves have destabilizing impacts or
    have misuse potential. Yet such safety impacts are not yet known or remain to
    be explored. In this paper, we outline a hazard analysis framework constructed
    at OpenAI to uncover hazards or safety risks that the deployment of models like
    Codex may impose technically, socially, politically, and economically. The
    analysis is informed by a novel evaluation framework that determines the
    capacity of advanced code generation techniques against the complexity and
    expressivity of specification prompts, and their capability to understand and
    execute them relative to human ability.
    Combined Score: 0.2109375
    Citation count: 27
    Year of publication: 2022
    Publication venue: arXiv.org
    Authors: H Khlaaf, P Mishkin, J Achiam, G Krueger


    Link: https://arxiv.org/pdf/2207.14157
    ArXiv Link: http://arxiv.org/pdf/2207.14157v1
    Downloading A hazard analysis framework for code synthesis large language models.pdf... with upper time limit: 10 seconds
    Downloaded: A hazard analysis framework for code synthesis large language models.pdf.

    The above displays all paper with a combined score no less than 0
    Metadata saved to papers/metadata.json

    Folder saved to papers.zip

    How would you like to summarize the papers?
    1. 'all': Summarize all papers in the organized folder.
    2. 'select': Choose specific papers by their ranks to summarize.

    Choose an option ('all' or 'select'): 
    Available papers with their ranks:
    1. 001_16.6_Is your code generated by chatgpt really correct rigorous evaluation of large language models for code generation.pdf
    2. 002_11.9_Self-planning code generation with large language models.pdf
    3. 003_10.2_A survey on large language models for code generation.pdf
    4. 004_5.4_A systematic evaluation of large language models of code.pdf
    5. 005_5.22_Program synthesis with large language models.pdf
    6. 006_3.29_Planning with large language models for code generation.pdf
    7. 007_1.79_Jigsaw Large language models meet program synthesis.pdf
    8. 008_0.795_A survey on evaluating large language models in code generation tasks.pdf
    9. 009_0.211_A hazard analysis framework for code synthesis large language models.pdf

    Enter the ranks of the papers you want to summarize, separated by commas (e.g., 1,3,5): 
    Summarizing the following papers: ['002_11.9_Self-planning code generation with large language models.pdf', '003_10.2_A survey on large language models for code generation.pdf']

    Processing file: 002_11.9_Self-planning code generation with large language models.pdf
    Begin analyzing the article located at papers/papers_organized/002_11.9_Self-planning code generation with large language models.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper introduces MultiPL-E, the first massively parallel, multi-language benchmark system aimed at evaluating code generation models across multiple programming languages by translating established Python benchmarks.

    2. Existing problems: Previous studies predominantly focused on evaluating code generation models using Python, limiting generalizability and failing to consider the performance of models across a diverse set of programming languages. Additionally, existing benchmarks lacked the complexity and representativeness needed for real-world applications and varied programming environments.

    3. The main contributions: The authors developed MultiPL-E to translate the unit test-driven code generation benchmarks HumanEval and MBPP into 18 different languages, facilitating a comparative analysis of code generation models such as Codex, CodeGen, and InCoder across diverse programming paradigms. This approach allows for scalable evaluation and is straightforward to extend for new languages and problems.

    4. Experimental results: The study evaluates the performance of three state-of-the-art models on the new multilingual benchmarks. Results show that Codex performs comparably to its Python performance across several languages, particularly excelling in JavaScript. The evaluation also highlights the effects of language features and frequency on model performance.

    5. Conclusions: The findings affirm that code generation models like Codex can effectively generalize across languages, exposing common error patterns in code generation that resemble those of human programmers. The paper emphasizes the importance of evaluating models with a diverse language set and offers a publicly available benchmark to aid future research in multi-language code generation models. Future work should focus on improving prompt designs and exploring the impact of specific programming language features on code generation performance.
    The total cost is 0.0036958499999999997 USD

    Processing file: 003_10.2_A survey on large language models for code generation.pdf
    Begin analyzing the article located at papers/papers_organized/003_10.2_A survey on large language models for code generation.pdf
    Summary information not found in storage
    Extracting from paper.
    ---extracting abstract---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting introduction---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting discussion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---extracting conclusion---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    ---summarizing---
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    The summary is:

    1. The main topic: The paper provides a systematic literature review focused on Large Language Models (LLMs) specifically for code generation, addressing the natural language-to-code (NL2Code) task and its advancements in software development.

    2. Existing problems: Previous studies have often lacked a comprehensive, up-to-date review dedicated solely to code generation by LLMs, with many existing surveys covering a broad range of code-related tasks rather than delving deeply into advanced topics and contemporary models.

    3. The main contributions: This paper introduces a taxonomy to categorize recent advances in LLMs for code generation, covering areas such as data curation, performance evaluation, and real-world applications. It also provides a historical overview of model development and identifies critical challenges and opportunities to bridge the gap between research and practical applications.

    4. Experimental results: The paper presents empirical comparisons using benchmarks such as HumanEval, MBPP, and BigCodeBench, showcasing progress in LLM capabilities across various programming tasks. It highlights the dramatic improvements in metrics like Pass@1, marking the evolution of performance from earlier models to state-of-the-art implementations.

    5. Conclusions: The findings indicate significant advancements in LLM capabilities for code generation, particularly in democratizing programming for non-experts. The survey suggests that continued exploration of challenging topics and practical applications is essential for bridging theoretical research and industry needs, advocating for ongoing documentation of advancements through a dedicated GitHub resource.
    The total cost is 0.00307935 USD

    Total cost for summarizing all files: 0.0067751999999999995

    The summaries for all selected files are printed below:
    ------Paper title: 002_11.9_Self-planning code generation with large language models.pdf------

    1. The main topic: The paper introduces MultiPL-E, the first massively parallel, multi-language benchmark system aimed at evaluating code generation models across multiple programming languages by translating established Python benchmarks.

    2. Existing problems: Previous studies predominantly focused on evaluating code generation models using Python, limiting generalizability and failing to consider the performance of models across a diverse set of programming languages. Additionally, existing benchmarks lacked the complexity and representativeness needed for real-world applications and varied programming environments.

    3. The main contributions: The authors developed MultiPL-E to translate the unit test-driven code generation benchmarks HumanEval and MBPP into 18 different languages, facilitating a comparative analysis of code generation models such as Codex, CodeGen, and InCoder across diverse programming paradigms. This approach allows for scalable evaluation and is straightforward to extend for new languages and problems.

    4. Experimental results: The study evaluates the performance of three state-of-the-art models on the new multilingual benchmarks. Results show that Codex performs comparably to its Python performance across several languages, particularly excelling in JavaScript. The evaluation also highlights the effects of language features and frequency on model performance.

    5. Conclusions: The findings affirm that code generation models like Codex can effectively generalize across languages, exposing common error patterns in code generation that resemble those of human programmers. The paper emphasizes the importance of evaluating models with a diverse language set and offers a publicly available benchmark to aid future research in multi-language code generation models. Future work should focus on improving prompt designs and exploring the impact of specific programming language features on code generation performance.



    ------Paper title: 003_10.2_A survey on large language models for code generation.pdf------

    1. The main topic: The paper provides a systematic literature review focused on Large Language Models (LLMs) specifically for code generation, addressing the natural language-to-code (NL2Code) task and its advancements in software development.

    2. Existing problems: Previous studies have often lacked a comprehensive, up-to-date review dedicated solely to code generation by LLMs, with many existing surveys covering a broad range of code-related tasks rather than delving deeply into advanced topics and contemporary models.

    3. The main contributions: This paper introduces a taxonomy to categorize recent advances in LLMs for code generation, covering areas such as data curation, performance evaluation, and real-world applications. It also provides a historical overview of model development and identifies critical challenges and opportunities to bridge the gap between research and practical applications.

    4. Experimental results: The paper presents empirical comparisons using benchmarks such as HumanEval, MBPP, and BigCodeBench, showcasing progress in LLM capabilities across various programming tasks. It highlights the dramatic improvements in metrics like Pass@1, marking the evolution of performance from earlier models to state-of-the-art implementations.

    5. Conclusions: The findings indicate significant advancements in LLM capabilities for code generation, particularly in democratizing programming for non-experts. The survey suggests that continued exploration of challenging topics and practical applications is essential for bridging theoretical research and industry needs, advocating for ongoing documentation of advancements through a dedicated GitHub resource.




    Would you like to check the code availability of the articles? (yes/no): 
    Checking code availability for the summarized articles...

    Checking code availability for: 002_11.9_Self-planning code generation with large language models.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    https://github.com/nuprl/MultiPL-E
    The total cost is 0.005967599999999999 USD

    Checking code availability for: 003_10.2_A survey on large language models for code generation.pdf
    Sequence generation under testing: attempt 1 of 3
    Operation under time limit: attempt 1 of 3
    The operation finishes in time
    Test passed
    The retrieved information is:

    https://github.com/juyongjiang/CodeLLMSurvey
    The total cost is 0.01090695 USD

    Total cost for checking code availability: 0.01687455 USD
    Total cost for the entire process (summaries + code availability check): 0.023649749999999997 USD






|

.. code-block:: Python


    from auto_research.applications.surveys import topic_to_survey


    if __name__ == "__main__":
        """
        Main execution block for the `topic_to_survey` function.

        This block initializes the `topic_to_survey` function with the specified parameters and runs the automated research process.

        Example:
            # Sample usage:
            topic_to_survey(
                num_results=5,
                sort_by="relevance",
                date_cutoff="2024-12-01",
                score_threshold=0,
                destination_folder="papers",
                model="gpt-4o-mini",
                api_key_path="",
                api_key_type="OpenAI",
                organize_files=True,
                order_by_score=True,
                zip_folder=True,
                api_key=None,  # Directly provide the API key as a string. If None, the key will be retrieved from the file.
            )

        Parameters
        ----------
        num_results : int, optional
            Number of search results to retrieve. Defaults to 30.
        sort_by : str, optional
            Sorting criteria for search results. Options: "relevance", "date". Defaults to "relevance".
        date_cutoff : str, optional
            Cutoff date for search results. Only articles published before this date will be included. Defaults to "2024-12-01". Only relevant when `sort_by` is set as "date".
        score_threshold : float, optional
            Minimum score threshold for articles. Articles with a score below this will be excluded. Defaults to 0.5.
        destination_folder : str, optional
            Folder to store downloaded articles. Defaults to "papers".
        model : str, optional
            Model to use for summarization and keyword suggestions. Defaults to "gpt-4o-mini".
        api_key_path : str, optional
            Path to the directory containing the API key. Defaults to "../". Set it as "" if the file is located at the current directory.
        api_key_type : str, optional
            Type of API key to retrieve. Options: "OpenAI", "DeepSeek". Defaults to "OpenAI".
        organize_files : bool, optional
            Whether to organize the downloaded articles into subfolders based on their rank and score. Defaults to True.
        order_by_score : bool, optional
            Whether to order articles by their score when organizing. Defaults to True.
        zip_folder : bool, optional
            Whether to zip the organized folder after processing. Defaults to True.
        api_key : str, optional
            Directly provide the API key as a string. If None, the key will be retrieved from the file. Defaults to None.

        Returns
        -------
        None
        """
        topic_to_survey(
            num_results=5,
            sort_by="relevance",
            date_cutoff="2024-12-01",
            score_threshold=0,
            destination_folder="papers",
            model="gpt-4o-mini",
            api_key_path="",
            api_key_type="OpenAI",
            organize_files=True,
            order_by_score=True,
            zip_folder=True,
            api_key=None,
        )

.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 53.159 seconds)


.. _sphx_glr_download__examples_gallery_top_to_survey.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: top_to_survey.ipynb <top_to_survey.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: top_to_survey.py <top_to_survey.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: top_to_survey.zip <top_to_survey.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
